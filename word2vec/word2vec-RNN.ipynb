{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#Kütüphanelerin eklenmesi\n",
    "import numpy as np #Bu kütüphane lineer cebir için kullandığımız kütüphane fonksiyonlarını içeriyor\n",
    "import pandas as pd # verilerimizi işlemek için pandas kütüphanasini kullanıyoruz(örn pd.read_scv)\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "import json\n",
    "import random\n",
    "#from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.tr import Turkish\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# DEEP LEARNING IMPORTS\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Activation, Dropout, Flatten, MaxPooling2D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column = ['tweets','duygu','preprocessing']\n",
    "#df = pd.read_excel(\"../dataset/total.xlsx\")\n",
    "\n",
    "column = ['tweets','duygu']\n",
    "df = pd.read_excel(\"../dataset/kemik_pos_neg.xlsx\")\n",
    "\n",
    "\n",
    "df.columns=column\n",
    "#veri setinin gösterilmesi\n",
    "df=df.drop_duplicates()\n",
    "df['tweets']=df['tweets'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>duygu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>turkcell heryerde çekiyor kesin bilgi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>turkcell olmak ayrıcalıktir çünkü kuzenlerin v...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>allahtan turkcell'liyim amin</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>avea kaşar yaşasın turkcell</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tweets duygu\n",
       "0              turkcell heryerde çekiyor kesin bilgi     1\n",
       "1  turkcell olmak ayrıcalıktir çünkü kuzenlerin v...     1\n",
       "2                       allahtan turkcell'liyim amin     1\n",
       "3                        avea kaşar yaşasın turkcell     1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.duygu==\"olumlu\",\"duygu\"]=1\n",
    "df.loc[df.duygu==\"olumsuz\",\"duygu\"]=0\n",
    "df[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweets</th>\n",
       "      <th>duygu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 saatir reklam veriyorsun ama hala çekmiyorsu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>turkcell grubu son 10 yilin geli̇r ve büyüme r...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>turkcell görme engelliler icin yaptiginiz rekl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>turkcell reklamındaki gibi bir sarkiyla kiz ta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hayret turkcell bana beleş 2500 sms verdi nolu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9121</th>\n",
       "      <td>new post: turkcell fatura itiraz dilekçesi örn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9122</th>\n",
       "      <td>turkcell sayesinde sabretmeyi öğrendim</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9123</th>\n",
       "      <td>turkcell 14 gb internet verdi ama sadece bir g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9124</th>\n",
       "      <td>kalktık ama halen internetteyim yaşa turkcell ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9125</th>\n",
       "      <td>turkcell'li degilim artik adamlar  mesaj atiyo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9126 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tweets duygu\n",
       "0     2 saatir reklam veriyorsun ama hala çekmiyorsu...     0\n",
       "1     turkcell grubu son 10 yilin geli̇r ve büyüme r...     1\n",
       "2     turkcell görme engelliler icin yaptiginiz rekl...     1\n",
       "3     turkcell reklamındaki gibi bir sarkiyla kiz ta...     0\n",
       "4     hayret turkcell bana beleş 2500 sms verdi nolu...     1\n",
       "...                                                 ...   ...\n",
       "9121  new post: turkcell fatura itiraz dilekçesi örn...     0\n",
       "9122             turkcell sayesinde sabretmeyi öğrendim     1\n",
       "9123  turkcell 14 gb internet verdi ama sadece bir g...     0\n",
       "9124  kalktık ama halen internetteyim yaşa turkcell ...     1\n",
       "9125  turkcell'li degilim artik adamlar  mesaj atiyo...     0\n",
       "\n",
       "[9126 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=df['tweets'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tokenizer\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "# Building word indices\n",
    "tokenizer.fit_on_texts(X_t)\n",
    "# Tokenizing sentences\n",
    "sentences = tokenizer.texts_to_sequences(X_t)\n",
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "# Creating texts \n",
    "X_t = list(map(sequence_to_text, sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.96999740600586\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "start=time.time()\n",
    "n_features=32\n",
    "window_size=5\n",
    "min_count=1\n",
    "epoch=50\n",
    "n_workers=8\n",
    "\n",
    "wordembeddings = Word2Vec(size = n_features,\n",
    "            window = window_size, \n",
    "            min_count= min_count,\n",
    "            workers = n_workers, \n",
    "            sg=1)\n",
    "wordembeddings.build_vocab(X_t)\n",
    "wordembeddings.train(X_t, \n",
    "            total_examples=wordembeddings.corpus_count,  \n",
    "            epochs = epoch)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=df['tweets'].to_numpy()\n",
    "targets=df['duygu'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec (X_train,y_train,X_test,y_test):\n",
    "    #Create a tokenizer, configured to only take into account the 20 most common words çok küçük olursa kelimeleri \n",
    "    #kaybederiz underfit yaparız\n",
    "    #tokenizer = Tokenizer(lower=True) #en yaygın kaç kelimeyi dikkate alacağı. Belirtilecek en iyi kelime sayısı #1000 yapan da var\n",
    "    tokenizer.fit_on_texts(X_train) #keras tokenizer ile metni dictionary haline getiriyor.\n",
    "    sequences_X_train = tokenizer.texts_to_sequences(X_train) #kelimelerin dictionarydeki karşılığı \n",
    "    #[[2, 1, 3], [2, 1], [4, 1], [5, 6]] şekline getiriliyor. 2-machine 1- learning 3-Knowledge \n",
    "    word_index = tokenizer.word_index #dictionarydeki kelimelerin sayısal karşılığı 'unk': 1, 'ürün': 2,\n",
    "    max_length = 0\n",
    "    for review_number in range(len(sequences_X_train)): #len(sequences_X_train) ile kaç tane [[2,3,4],[2,6]] var bulunuyor burda 2\n",
    "        numberofwords=len(sequences_X_train[review_number]) #[2,3,4] içinde kaç tane şey var 3 burda\n",
    "        if (numberofwords) > (max_length):\n",
    "            max_length = numberofwords #tüm kelimelere bakıp en uzun olanı buluyor\n",
    "\n",
    "    X_train = pad_sequences(sequences_X_train, maxlen=max_length) #ikili boyutlu matrise çevirip her cümelnin uzunluğunu eşit yapıyor.\n",
    "    #En uzun cümle uzunluğuna tamamlanıyor.[[2 1 3] [0 2 1]] alt alta gelecek şekilde en uzun 6 ise 6x6 matris oluyor\n",
    "    y_train = np.asarray(y_train) #tek boyutlu bir matris oluyor [1 1 0 ... 0 1 0] gibi\n",
    "\n",
    "    sequences_X_test = tokenizer.texts_to_sequences(X_test) #train için yapılan gibi dictionary alınıyor\n",
    "    X_test = pad_sequences(sequences_X_test, maxlen=max_length) #en uzun olana göre pad sequence yapılıyor\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "    unique_words = len(word_index) #word_index ile unique olan kelimeler alınıyor 0 dan başladığı için bir arttırılıyor\n",
    "    total_words = unique_words + 1\n",
    "    skipped_words = 0\n",
    "    embedding_dim = 32 #embedding dim vector size ile aynı \n",
    "    embedding_vector=0\n",
    "    embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "    for word, index in tokenizer.word_index.items(): #kelime ve kelimenin dictionarydeki karşılığı alınıyor\n",
    "        try:\n",
    "            embedding_vector = wordembeddings[word]#wordembeddings.word_vectors[wordembeddings.dictionary[word]] #kelimenin word2vec karşılığı vektör olarak\n",
    "\n",
    "        except:\n",
    "            skipped_words = skipped_words+1\n",
    "            pass\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector #dictionarydeki indexine word2vec teki sayısal hali yazılır\n",
    "            \n",
    "    embedding_layer = Embedding(total_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    \n",
    "    return embedding_layer,X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-7d0886ed6bc1>:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  embedding_vector = wordembeddings[word]#wordembeddings.word_vectors[wordembeddings.dictionary[word]] #kelimenin word2vec karşılığı vektör olarak\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------\n",
      "Training for fold 1 ...\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 12s 46ms/step - loss: 0.5213 - acc: 0.7398 - precision_1: 0.7480 - recall_1: 0.7279\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 11s 45ms/step - loss: 0.4610 - acc: 0.7812 - precision_1: 0.7928 - recall_1: 0.7649\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 14s 54ms/step - loss: 0.4272 - acc: 0.8012 - precision_1: 0.8162 - recall_1: 0.7804\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 13s 49ms/step - loss: 0.4000 - acc: 0.8237 - precision_1: 0.8360 - recall_1: 0.8080\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 16s 61ms/step - loss: 0.3568 - acc: 0.8406 - precision_1: 0.8526 - recall_1: 0.82600s - loss: 0.3563 - acc: 0.8410 - precision_1: 0.8526 - recall_1\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 22s 86ms/step - loss: 0.3244 - acc: 0.8589 - precision_1: 0.8664 - recall_1: 0.8506: 13s - loss: 0.3184 - acc: 0.8690 - precision_1: 0.8801  - ETA: 12s - loss: 0.3094 - acc: 0.8686 - prec - ETA: 10s - loss: 0.2993 - acc: 0.8741 - precisio - ETA: 1s - loss: 0.3222 - acc:\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.2711 - acc: 0.8877 - precision_1: 0.8932 - recall_1: 0.8824: 15s - loss: 0.3334 - acc: 0.8542 - precision_1: - ETA: 20s - loss: 0.2515 - acc: 0.9030 - precisio - ETA: 10s - \n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.2232 - acc: 0.9129 - precision_1: 0.9174 - recall_1: 0.9087\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.1754 - acc: 0.9319 - precision_1: 0.9329 - recall_1: 0.9317\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.1370 - acc: 0.9473 - precision_1: 0.9486 - recall_1: 0.9465\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.1155 - acc: 0.9592 - precision_1: 0.9591 - recall_1: 0.95980s - loss: 0.1146 - acc: 0.9597 - precision_1: 0.9598 - reca\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 25s 96ms/step - loss: 0.0951 - acc: 0.9679 - precision_1: 0.9683 - recall_1: 0.9678\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.0756 - acc: 0.9733 - precision_1: 0.9743 - recall_1: 0.97 - 24s 93ms/step - loss: 0.0756 - acc: 0.9733 - precision_1: 0.9743 - recall_1: 0.9726\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0887 - acc: 0.9694 - precision_1: 0.9681 - recall_1: 0.9712TA: 2s - l\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0642 - acc: 0.9778 - precision_1: 0.9789 - recall_1: 0.9770\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0629 - acc: 0.9782 - precision_1: 0.9775 - recall_1: 0.97920s - loss: 0.0630 - acc: 0.9781 - precision_1: 0.9774 - recall_1: \n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0570 - acc: 0.9799 - precision_1: 0.9818 - recall_1: 0.9782\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0565 - acc: 0.9828 - precision_1: 0.9835 - recall_1: 0.9823\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0536 - acc: 0.9822 - precision_1: 0.9826 - recall_1: 0.9821\n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0446 - acc: 0.9860 - precision_1: 0.9874 - recall_1: 0.9847\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 24s 95ms/step - loss: 0.0478 - acc: 0.9839 - precision_1: 0.9857 - recall_1: 0.9823: 15s - loss: 0.0467 - acc: 0.9832 - precisio\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0505 - acc: 0.9822 - precision_1: 0.9828 - recall_1: 0.98183s - - ETA: 0s - loss: 0.0501 - acc: 0.9825 - precision_1: 0.9835 - recall\n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0421 - acc: 0.9856 - precision_1: 0.9838 - recall_1: 0.9877: 21s - loss: 0.0339 - acc: 0 - ETA: 3s - loss: 0.0401 -  - ETA: 0s - loss: 0.0410 - acc: 0.9861 - precision_1: 0.9846 \n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 22s 86ms/step - loss: 0.0341 - acc: 0.9893 - precision_1: 0.9903 - recall_1: 0.9884\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.0635 - acc: 0.9788 - precision_1: 0.9815 - recall_1: 0.9763\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 19s 72ms/step - loss: 0.0416 - acc: 0.9872 - precision_1: 0.9865 - recall_1: 0.9881\n",
      "Epoch 27/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.0258 - acc: 0.9920 - precision_1: 0.9932 - recall_1: 0.9908\n",
      "Epoch 28/50\n",
      "257/257 [==============================] - 21s 82ms/step - loss: 0.0376 - acc: 0.9875 - precision_1: 0.9884 - recall_1: 0.9867\n",
      "Epoch 29/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0575 - acc: 0.9812 - precision_1: 0.9825 - recall_1: 0.98021s - loss: 0.0575 - acc: 0.9812 - precision_1: 0.9824 - recall_1: 0.98 - ETA: 1s - loss: 0.0575 - acc: 0.9812 - prec\n",
      "Epoch 30/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0620 - acc: 0.9800 - precision_1: 0.9802 - recall_1: 0.9802: 19s - loss: 0.0376 - acc: 0.9908 - precisio - ETA: 16s - loss: 0.0370 - acc: 0.9899 - precision_1: 0.9877 - re - ETA: 15s -  - ETA: 9s - loss: 0 - ETA\n",
      "Epoch 31/50\n",
      "257/257 [==============================] - 17s 67ms/step - loss: 0.0399 - acc: 0.9882 - precision_1: 0.9881 - recall_1: 0.9884\n",
      "Epoch 32/50\n",
      "257/257 [==============================] - 16s 64ms/step - loss: 0.0185 - acc: 0.9939 - precision_1: 0.9939 - recall_1: 0.9939\n",
      "Epoch 33/50\n",
      "257/257 [==============================] - 17s 66ms/step - loss: 0.0195 - acc: 0.9938 - precision_1: 0.9935 - recall_1: 0.99420s - loss: 0.0193 - acc: 0.9941 - precision_1: 0.9943 - \n",
      "Epoch 34/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.0604 - acc: 0.9789 - precision_1: 0.9810 - recall_1: 0.9770 9s - loss:\n",
      "Epoch 35/50\n",
      "257/257 [==============================] - 21s 81ms/step - loss: 0.4551 - acc: 0.7836 - precision_2: 0.7935 - recall_2: 0.7659\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 18s 72ms/step - loss: 0.4189 - acc: 0.8059 - precision_2: 0.8137 - recall_2: 0.7927\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.3833 - acc: 0.8312 - precision_2: 0.8379 - recall_2: 0.8207\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.3376 - acc: 0.8534 - precision_2: 0.8595 - recall_2: 0.8444\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 22s 85ms/step - loss: 0.2898 - acc: 0.8775 - precision_2: 0.8812 - recall_2: 0.87226s - loss: 0.2812 - acc: 0.8824 - precision_2: 0.8884  - ETA: \n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.2363 - acc: 0.9045 - precision_2: 0.9066 - recall_2: 0.9017\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.2020 - acc: 0.9206 - precision_2: 0.9228 - recall_2: 0.9178\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 24s 95ms/step - loss: 0.1553 - acc: 0.9448 - precision_2: 0.9425 - recall_2: 0.9473: 20s - loss: 0.1286 - acc: 0.9641 - precision_2: 0.9653 - r - ETA: 20s - loss: 0.1215 - acc: 0.9670 - precision_2: 0.9632 - - ETA: 19s - loss: 0.1361 - acc: 0 - ETA: 14s - loss: 0.1499 - acc: 0.9522 - precision_2: 0.9518 - recall - ETA: 0s - loss: 0.1554 - acc: 0.9448 - precision_2: 0.9426 - re\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.1162 - acc: 0.9569 - precision_2: 0.9555 - recall_2: 0.95833s -\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.4604 - acc: 0.7834 - precision_3: 0.7898 - recall_3: 0.7743: 19s - loss: 0.0933 - - ETA: 15s - loss: 0.0922 - acc: 0.9684 - precision_\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.4222 - acc: 0.8051 - precision_3: 0.8198 - recall_3: 0.78370s - loss: 0.4214 - acc: 0.8053 - precision_3: 0.8195 - re\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 24s 93ms/step - loss: 0.3944 - acc: 0.8230 - precision_3: 0.8376 - recall_3: 0.8027\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.2188 - acc: 0.9112 - precision_3: 0.9144 - recall_3: 0.9080: 19s - loss: 0 - ETA: 1s - loss: 0.2196 - acc: 0.9107 - precision_3: 0.9148 - recall - ETA: 1s - loss: 0.2195 - acc: 0.9105 - precis\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.1795 - acc: 0.9312 - precision_3: 0.9366 - recall_3: 0.92554s - loss: 0.1792 - acc: 0.9296 -  - ETA: 2s - loss: 0.1\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.1291 - acc: 0.9517 - precision_3: 0.9502 - recall_3: 0.9536\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.1204 - acc: 0.9543 - precision_3: 0.9559 - recall_3: 0.9529\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.0980 - acc: 0.9659 - precision_3: 0.9656 - recall_3: 0.9665\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.0861 - acc: 0.9681 - precision_3: 0.9673 - recall_3: 0.9692\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0534 - acc: 0.9843 - precision_3: 0.9847 - recall_3: 0.9840\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0650 - acc: 0.9781 - precision_3: 0.9779 - recall_3: 0.9784\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0544 - acc: 0.9806 - precision_3: 0.9815 - recall_3: 0.97991s - loss: 0.0529 - acc: 0.9814 - precision_\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0850 - acc: 0.9704 - precision_3: 0.9693 - recall_3: 0.9718\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0488 - acc: 0.9848 - precision_3: 0.9835 - recall_3: 0.9862: 20s - loss: 0.0496 - acc: 0.9873 - precision_3: 0.9863 - recal - ETA: 19s - loss: 0.0590 - acc: 0.9811 - precision_ - ETA: 16s - loss: 0.0536 - acc: 0.9828 - precision_3: 0.9812 - recall - ETA: 16 - ETA: 2s - loss: 0.0506 - acc: 0.9841 - precision_3: - ETA: 1s - loss: 0.0494 - acc: 0.9846 - precisio\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.0374 - acc: 0.9870 - precision_3: 0.9864 - recall_3: 0.98761s - loss: 0.0356 - acc: 0.9875 \n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.0709 - acc: 0.9755 - precision_3: 0.9748 - recall_3: 0.97650s - loss: 0.0703 - acc: 0.9759 - precision_3: 0.9748 - reca\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0459 - acc: 0.9838 - precision_3: 0.9833 - recall_3: 0.9845: 18s - loss: 0.0602 - acc:  - ETA: 8s - loss: 0.0499 - acc: 0.9828 -  - ETA: 6s - loss: 0.0488 - acc: 0.9828 - precision_3: 0 - ETA: 5s - loss: 0.0482 - acc: 0.9831 - precision_3: 0.9825 - re - ETA: 4s - loss: 0.0479 - acc: 0.9832 - precision_3: - ETA: 3s - loss: 0.0465 - acc: 0.9838 - precision_3: 0.9 - ETA: 2s - loss: 0.0466 - acc: 0.983 - ETA: 0s - loss: 0.0458 - acc: 0.9840 - precision_3: 0.983\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0436 - acc: 0.9847 - precision_3: 0.9842 - recall_3: 0.9852: 19s - loss - ETA: 15s - loss: 0.0365 - acc: 0.9\n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.0340 - acc: 0.9893 - precision_3: 0.9896 - recall_3: 0.9891: 20s - loss: 0.0136 - acc: 0.9974 - prec\n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.0514 - acc: 0.9850 - precision_3: 0.9852 - recall_3: 0.9850: 20s - loss: 0.0412 - acc: 0.9860 - precision_3: - ETA: 17s - loss: 0.0444 - acc: 0.9888 - precision_3: 0.9877 - reca - ETA: 16s - loss: 0.0448 - ac\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.0332 - acc: 0.9895 - precision_3: 0.9877 - recall_3: 0.9915:  - ETA: 9s - ETA: 6s - l\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.0127 - acc: 0.9968 - precision_3: 0.9971 - recall_3: 0.99662s - loss: 0.0136 - acc: 0.9965 - precision_ - ETA: 1s - loss: 0.0132 - acc: 0.9967 - precision_3:\n",
      "Epoch 27/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.0145 - acc: 0.9955 - precision_3: 0.9956 - recall_3: 0.9954\n",
      "Epoch 28/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.0855 - acc: 0.9720 - precision_3: 0.9721 - recall_3: 0.9721\n",
      "Epoch 29/50\n",
      "257/257 [==============================] - 15s 58ms/step - loss: 0.0719 - acc: 0.9753 - precision_3: 0.9748 - recall_3: 0.9760\n",
      "Epoch 30/50\n",
      "257/257 [==============================] - 15s 58ms/step - loss: 0.0359 - acc: 0.9883 - precision_3: 0.9886 - recall_3: 0.9881\n",
      "Epoch 31/50\n",
      "257/257 [==============================] - 18s 69ms/step - loss: 0.0261 - acc: 0.9906 - precision_3: 0.9903 - recall_3: 0.9910\n",
      "Epoch 00031: early stopping\n",
      "29/29 [==============================] - 1s 32ms/step - loss: 1.2595 - acc: 0.7590 - precision_3: 0.7234 - recall_3: 0.8149\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 4 ...\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.5362 - acc: 0.7265 - precision_4: 0.7371 - recall_4: 0.7041\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.4665 - acc: 0.7805 - precision_4: 0.7909 - recall_4: 0.7625\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 18s 70ms/step - loss: 0.4296 - acc: 0.8017 - precision_4: 0.8139 - recall_4: 0.7820\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 22s 87ms/step - loss: 0.4052 - acc: 0.8185 - precision_4: 0.8306 - recall_4: 0.8000: 16s - loss: 0.3739 - acc: 0.8363 - precision_4: 0.8490 - recall_4: 0 - ETA: 15s - loss: 0.3728 - acc: 0.8 - ETA: 13s - loss: 0.3789 - acc: 0.8321 - precision_4: 0.8379 - recal - ETA - ETA: 1s - loss: 0.4012 - acc: 0.8212 - precisio - ETA: 0s - loss: 0.4050 - acc: 0.8186 - precision_4: 0.8310 - recall_4: 0.\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.3726 - acc: 0.8392 - precision_4: 0.8461 - recall_4: 0.82908s - loss: 0.3794 - acc: 0.8369 -  - ETA: 2s - loss: - ETA: 0s - loss: 0.3730 - acc: 0.8387 - precision_4: 0.8455 - recall_4: 0. - ETA: 0s - loss: 0.3726 - acc: 0.8392 - precision_4: 0.8461 - recall_4: 0.82\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - ETA: 0s - loss: 0.3163 - acc: 0.8641 - precision_4: 0.8706 - recall_4: 0.8553 - ETA: 17s - loss: 0.2937 - acc: 0.8 - ETA: 13s - loss: 0.3069 - acc: 0.8690 - precision_4: 0.8753 - recall_4: - ETA: - ETA: 0s - loss: 0.3171 - acc: 0.8639 - precision_4: 0.8696 - recall - 23s 91ms/step - loss: 0.3163 - acc: 0.8641 - precision_4: 0.8706 - recall_4: 0.8553\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.2802 - acc: 0.8829 - precision_4: 0.8899 - recall_4: 0.8738\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.2238 - acc: 0.9092 - precision_4: 0.9142 - recall_4: 0.9031: 16s - loss: 0.1795 - acc: 0.9324 - prec - ETA: 13s - loss: 0.2035 - acc: 0.9177 - precision_4: 0.9193 - recall_4: - ETA: 13s - loss: 0.2047 - acc - ETA: 9s - loss: 0.2115 - acc: - ETA: 7s - loss: 0.2160 - acc: 0.9128 - precision_4: 0.9161 - recall - ETA\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.1745 - acc: 0.9341 - precision_4: 0.9366 - recall_4: 0.9313: 10s - l\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.1483 - acc: 0.9419 - precision_4: 0.9446 - recall_4: 0.9389: 20s - loss: 0.1251 - acc: 0.9544 - ETA: 17s - loss: 0.1449 - acc: 0.9438 - precision_4: 0.949 - ETA: 15s - loss: 0.1415 - a - ETA: 11s - loss: 0.1 - ETA: 4s - loss: 0.1483 -  - ETA: 1s - loss: 0.1477 - acc: 0.942\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.1030 - acc: 0.9642 - precision_4: 0.9672 - recall_4: 0.9610\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0943 - acc: 0.9659 - precision_4: 0.9686 - recall_4: 0.9630: 18s - loss: 0 - ETA: 17s - loss: 0.0\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0866 - acc: 0.9709 - precision_4: 0.9724 - recall_4: 0.9693: 21s - loss: 0.0616 - acc: 0.9661 - precision_4: 0.9534 - recall_ - ETA: 20s - loss: 0.0560 - acc: 0.9762 - precision_4:  - ETA: 19s - loss: 0.0577 - acc: 0.9803  - ETA: 5s - loss: 0.0683 - acc: 0.9773 - precision_4: 0.9 - E - ETA: 0s - loss: 0.0852 - acc: 0.9718 - precision_4: 0.9745 -  - ETA: 0s - loss: 0.0863 - acc: 0.9712 - precision_4: 0.9731 - recall_4: \n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.0831 - acc: 0.9691 - precision_4: 0.9695 - recall_4: 0.9686: 19s - loss: 0.0639 - acc: 0.9757 -\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.0604 - acc: 0.9826 - precision_4: 0.9846 - recall_4: 0.9805\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0469 - acc: 0.9836 - precision_4: 0.9872 - recall_4: 0.97981s - loss: 0.0463 - acc: 0.9839 - precision_4:\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0313 - acc: 0.9915 - precision_4: 0.9912 - recall_4: 0.9917: 15s -  - ETA: 2s - loss: 0.031 - ETA: 0s - loss: 0.0314 - acc: 0.9915 - precision_4: 0.9912 - recall_4: 0.\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0446 - acc: 0.9859 - precision_4: 0.9849 - recall_4: 0.9868\n",
      "Epoch 19/50\n",
      "29/29 [==============================] - 1s 27ms/step - loss: 0.9405 - acc: 0.7952 - precision_4: 0.8154 - recall_4: 0.7637ETA: 21s - loss: 0.0431 - acc: 0.9 - ETA: 0s - loss: 0.9791 - acc: 0.7958 - precision_4: 0.8341 - re\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 5 ...\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 19s 72ms/step - loss: 0.5274 - acc: 0.7381 - precision_5: 0.7436 - recall_5: 0.7252\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 23s 88ms/step - loss: 0.4552 - acc: 0.7875 - precision_5: 0.7969 - recall_5: 0.7706\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.4235 - acc: 0.8086 - precision_5: 0.8183 - recall_5: 0.79234s - loss: 0.4259 - acc: 0.8064 - precision_5: 0.8 - ETA\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.3834 - acc: 0.8308 - precision_5: 0.8405 - recall_5: 0.8155: 17s - loss: \n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 17s 65ms/step - loss: 0.3433 - acc: 0.8494 - precision_5: 0.8543 - recall_5: 0.8416: 20s - loss: 0.3367 - acc: 0.8655 - precision_5: 0.8657 - recall_5: 0.85 - ETA: 20s - loss: 0.3441 - acc: 0.8625 - precision_5 - ETA: 15s - loss: 0.3220 - acc: 0.8666 - precision_5: 0.8714 - recall_5: 0.85 - ETA: 15s - loss: 0.3244 - acc: 0.8657 - precision_5:\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 15s 60ms/step - loss: 0.2902 - acc: 0.8791 - precision_5: 0.8834 - recall_5: 0.8729: 13s - loss: 0.2667 - acc: 0.8998 - precision_5: 0.8983 - rec - ETA: 12s - loss: 0.2744 - acc: 0.8910 - p - ETA: 0s - loss: 0.2904 - acc: 0.8784 - precision_5: 0.8831 \n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 16s 61ms/step - loss: 0.2368 - acc: 0.9039 - precision_5: 0.9072 - recall_5: 0.89956s - loss: 0.221 - ETA: 2s - loss: 0.2282 - acc: 0.9 - ETA: 1s - loss: 0.2340 - acc: 0.9044 - \n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.1980 - acc: 0.9182 - precision_5: 0.9190 - recall_5: 0.9168: 18s - loss: 0.1772 - acc: 0.9306  - ETA: 15s - loss: 0.1861 - acc: 0. - ETA: 12s - loss: 0.1976 - acc: 0.9229 - precision_5: 0.9243 - recall_5: 0 - ETA: 11s - loss: 0.1958 - acc: 0. - ETA: 1s - loss: 0.1998 - acc: 0.9174 - precis\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.1487 - acc: 0.9455 - precision_5: 0.9480 - recall_5: 0.9424\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.1162 - acc: 0.9590 - precision_5: 0.9581 - recall_5: 0.9597: 20s - loss: 0.0933 - acc: 0.9732 - precision_5: 0.9702 - rec - ETA: 20s - loss: 0.0909 - acc: 0.9724 - precision_5: - ETA: 18s - loss: 0.0961 - acc: 0.9724 - precisio - ETA: 15s - lo - ETA: 5s - loss: 0.1078 - acc: 0.9629 - precision_5: 0.9 - ETA: 4s - loss: 0.1082 - acc: 0.9624 - precision_5: 0.9614 - recall_5: 0. - ETA: 4s - loss: 0.1080  - ETA: 2s - loss: 0.1138 - \n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.1039 - acc: 0.9627 - precision_5: 0.9622 - recall_5: 0.96322s - los\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0811 - acc: 0.9728 - precision_5: 0.9727 - recall_5: 0.9729: 19s - loss: 0.0620 - acc: 0.9829 - precision_5: 0.9825 - recall_5: - ETA: 2s - loss: 0.0803 - acc: 0.9735 - precision_5: 0.9728 -  - ETA: 2s - loss: 0.0800 - acc:\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0566 - acc: 0.9798 - precision_5: 0.9781 - recall_5: 0.9815: 20s - loss: 0.0581 - acc: 0.9708 - precisi\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.0697 - acc: 0.9741 - precision_5: 0.9739 - recall_5: 0.9741: 17s - loss: 0.0385 - acc: 0.9876 - precision_5: 0.9903 - recall_5: 0.98 - ETA: 17s - loss: 0.0389 - acc: 0.9875 - precision_5: 0.989 - ETA: 15s - loss: 0.0380 - acc: 0.9871 - precision_5: 0.9899 - recall_5: 0.98 - ETA: 15s - loss: 0.0380 - ETA: 2s - loss: 0.0655 - acc: 0.9766 - precision_5: 0.9768 - reca - ETA: 2s - loss: 0.0658 - acc: 0.9762 - precis - ETA: 0s - loss: 0.0693 - acc: 0.9745 - precision_5: 0.9743 - re - ETA: 0s - loss: 0.0700 - acc: 0.9739 - precision_5: 0.9737 - recall_5: \n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.0567 - acc: 0.9802 - precision_5: 0.9807 - recall_5: 0.9795\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0636 - acc: 0.9788 - precision_5: 0.9776 - recall_5: 0.9800 - ETA: 0s - loss: 0.0625 - acc: 0.9790 - precision_5: 0.9\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0462 - acc: 0.9853 - precision_5: 0.9837 - recall_5: 0.9868\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0495 - acc: 0.9838 - precision_5: 0.9848 - recall_5: 0.9827\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0505 - acc: 0.9825 - precision_5: 0.9827 - recall_5: 0.9822\n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.0538 - acc: 0.9831 - precision_5: 0.9834 - recall_5: 0.9827\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 18s 72ms/step - loss: 0.0278 - acc: 0.9915 - precision_5: 0.9903 - recall_5: 0.9927\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.0377 - acc: 0.9869 - precision_5: 0.9854 - recall_5: 0.9883\n",
      "Epoch 27/50\n",
      "257/257 [==============================] - 20s 78ms/step - loss: 0.0434 - acc: 0.9861 - precision_5: 0.9861 - recall_5: 0.9861\n",
      "Epoch 28/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0378 - acc: 0.9862 - precision_5: 0.9868 - recall_5: 0.9856\n",
      "Epoch 29/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.0413 - acc: 0.9879 - precision_5: 0.9869 - recall_5: 0.98900s - loss: 0.0413 - acc: 0.9879 - precision_5: 0.9869 - recall_5: 0.98\n",
      "Epoch 30/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0153 - acc: 0.9953 - precision_5: 0.9958 - recall_5: 0.9946\n",
      "Epoch 31/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0508 - acc: 0.9821 - precision_5: 0.9831 - recall_5: 0.9810\n",
      "Epoch 32/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0535 - acc: 0.9802 - precision_5: 0.9809 - recall_5: 0.979310s - loss: 0.0347 - acc: 0.9688 - precision_5: 0.9697 - recal - ETA: 21s - loss: 0.0477 - ETA: 16s - loss: 0\n",
      "Epoch 33/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0266 - acc: 0.9925 - precision_5: 0.9927 - recall_5: 0.99222s - loss: 0.0267 - acc: 0.9928 - precision_5: 0.9939 - reca - ETA: 2s - loss: 0.0266 - ac\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0185 - acc: 0.9953 - precision_5: 0.9956 - recall_5: 0.9949: 21s - loss: 0.0 - ETA: 2s - loss: 0.0170 - \n",
      "Epoch 35/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0426 - acc: 0.9851 - precision_5: 0.9861 - recall_5: 0.98415s - loss: 0.0462 - acc: 0.9835 - precision_5: 0.9844 - recall_5: 0.98 - ETA: 5s - loss: 0.0462 - acc:\n",
      "Epoch 00035: early stopping\n",
      "29/29 [==============================] - 1s 33ms/step - loss: 1.3242 - acc: 0.7853 - precision_5: 0.7892 - recall_5: 0.7892\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 6 ...\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.5262 - acc: 0.7413 - precision_6: 0.7481 - recall_6: 0.7263: 16s - loss: 0.5719 - acc: 0.7105 - precision_6: 0.7146 - r - ETA: 14s - ETA: 1s - loss: 0.5315 - ac\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 16s 61ms/step - loss: 0.4634 - acc: 0.7836 - precision_6: 0.7974 - recall_6: 0.75950s - loss: 0.4634 - acc: 0.7836 - precision_6: 0.7974 - recall_6: 0.75\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 16s 61ms/step - loss: 0.4300 - acc: 0.8030 - precision_6: 0.8140 - recall_6: 0.7846\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.4009 - acc: 0.8187 - precision_6: 0.8288 - recall_6: 0.8027\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.3660 - acc: 0.8360 - precision_6: 0.8461 - recall_6: 0.8207: 21s - loss: 0.3203 - acc: 0.8661 - precision_6: - ETA: 19s - - ETA: 0s - loss: 0.3636 - acc: 0.8366 - precision_6: 0.8469 - \n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.3200 - acc: 0.8622 - precision_6: 0.8734 - recall_6: 0.8466: 20s - loss: 0.2950 - acc - ETA: 18s - \n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.2790 - acc: 0.8819 - precision_6: 0.8868 - recall_6: 0.8751: 21s - loss: 0.2499 - acc: 0.8918 - precision_6: 0.8756 - recall_ - ETA: 8s - loss: 0.2816 - acc: 0.8812 - precision_6: 0.8833 -  - ETA: 7s - loss: 0.2791 - acc: 0.8811 - precision_6: 0.8832 - recall - ETA:  - ETA: 3s - l - ETA: 0s - loss: 0.2784 - acc: 0.8823 - precision_6: 0.8851 \n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 23s 90ms/step - loss: 0.2371 - acc: 0.9036 - precision_6: 0.9092 - recall_6: 0.8963\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.1946 - acc: 0.9244 - precision_6: 0.9279 - recall_6: 0.9200\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.1630 - acc: 0.9399 - precision_6: 0.9408 - recall_6: 0.9385\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.1228 - acc: 0.9534 - precision_6: 0.9552 - recall_6: 0.9512\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 21s 82ms/step - loss: 0.1083 - acc: 0.9581 - precision_6: 0.9607 - recall_6: 0.95516s - loss: 0.0964 - acc: 0.9640  - ETA: 4s - loss: 0.099 - ETA: 2s - loss: 0.1039 - acc: 0.9609 - precision_6: 0.9633 - recall - ETA: 2s - loss: 0.1042 \n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.1033 - acc: 0.9664 - precision_6: 0.9695 - recall_6: 0.96294s - loss: 0.1006 - acc: 0.9680 - precisio\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0637 - acc: 0.9793 - precision_6: 0.9797 - recall_6: 0.9788\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0675 - acc: 0.9764 - precision_6: 0.9780 - recall_6: 0.97468s - l - ETA: 1s - loss: 0.0654 - acc: 0.976\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0735 - acc: 0.9758 - precision_6: 0.9784 - recall_6: 0.97295s - loss: 0.0683 - ac - ETA: 2s - loss:\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0590 - acc: 0.9808 - precision_6: 0.9805 - recall_6: 0.9810\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0592 - acc: 0.9793 - precision_6: 0.9793 - recall_6: 0.9793\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 24s 95ms/step - loss: 0.0597 - acc: 0.9810 - precision_6: 0.9803 - recall_6: 0.98171s - loss: 0.0580 - acc: 0.9823 \n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0679 - acc: 0.9770 - precision_6: 0.9766 - recall_6: 0.97730s - loss: 0.0686 - acc: 0.9769 - precision_6: 0.9762 \n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 24s 95ms/step - loss: 0.0290 - acc: 0.9912 - precision_6: 0.9924 - recall_6: 0.99006s - - ETA: 3s -\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0423 - acc: 0.9870 - precision_6: 0.9880 - recall_6: 0.98590s - loss: 0.0422 - acc: 0.9873 - precision_6: 0.9884 - recall_6: \n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0584 - acc: 0.9802 - precision_6: 0.9809 - recall_6: 0.9793: 21 - ETA: 15s - loss: 0.0698 - acc: 0.9754 - p\n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 24s 95ms/step - loss: 0.0501 - acc: 0.9832 - precision_6: 0.9825 - recall_6: 0.9839\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0363 - acc: 0.9878 - precision_6: 0.9885 - recall_6: 0.9871\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 24s 95ms/step - loss: 0.0259 - acc: 0.9927 - precision_6: 0.9939 - recall_6: 0.9915: 19s - loss: 0.0406 - acc: 0.9955 - pre - ETA: 12s - loss: 0.0255 - acc: 0.9931 - precision_6: 0.9943 - recall_ - ETA: 1\n",
      "Epoch 27/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0330 - acc: 0.9892 - precision_6: 0.9909 - recall_6: 0.9873: 19s - loss: 0.0310 - acc: 0.9915 - precisio - ETA: 1s - loss: 0.0315 - acc: 0.9899 - \n",
      "Epoch 28/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0400 - acc: 0.9867 - precision_6: 0.9885 - recall_6: 0.9849\n",
      "Epoch 29/50\n",
      "257/257 [==============================] - 19s 76ms/step - loss: 0.0719 - acc: 0.9769 - precision_6: 0.9789 - recall_6: 0.9746\n",
      "Epoch 30/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.0432 - acc: 0.9855 - precision_6: 0.9847 - recall_6: 0.9863\n",
      "Epoch 31/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.0428 - acc: 0.9858 - precision_6: 0.9873 - recall_6: 0.9841\n",
      "Epoch 00031: early stopping\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 1.3861 - acc: 0.7437 - precision_6: 0.7786 - recall_6: 0.6911\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 7 ...\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 15s 60ms/step - loss: 0.5321 - acc: 0.7328 - precision_7: 0.7397 - recall_7: 0.7190\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 15s 60ms/step - loss: 0.4577 - acc: 0.7839 - precision_7: 0.7923 - recall_7: 0.77011s - loss: 0.4573 - acc: 0.7847 - precision_7: 0.7941 - recall_7 - ETA: 1s - loss: 0.4577 - acc: 0.7844 - \n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 21s 82ms/step - loss: 0.4269 - acc: 0.8030 - precision_7: 0.8115 - recall_7: 0.7898\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.3945 - acc: 0.8219 - precision_7: 0.8276 - recall_7: 0.8137\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 23s 91ms/step - loss: 0.3404 - acc: 0.8532 - precision_7: 0.8598 - recall_7: 0.84436s - loss: 0.3429 - acc: 0.8515 - precision_7: 0 - ETA: 5s - loss: 0.3401 - acc: 0.8534 - precision_7: 0.8621 - re - ETA: 4s - loss: 0.3405 - acc: 0.8529 - precisio - ETA: 3s - loss: 0.3417 - acc: 0.8519 -  - ETA: 1s - loss: 0.3407 - acc: 0.8523 - precis\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 23s 91ms/step - loss: 0.3043 - acc: 0.8738 - precision_7: 0.8731 - recall_7: 0.8750: 15s - loss: 0.2772 - acc: 0.8902 - precision_7: 0.8852 - recall_7: 0 - ETA: 15s - loss: 0.2817 - acc: 0.8882 - precision_7:  - ETA: 13s - loss: 0.2876 - acc: 0.8847 - precision_7: 0. - ETA: 11s - loss: 0.2924 - acc: 0.8816 - precision_7: 0.8797 - recall_7: 0.8 - ETA: 10s - loss: 0.2927 - acc: 0.8810 - precision_7: 0.8781 - ETA: 9s - loss: 0.2986 - acc: 0.8777 -  - ETA: 7s - loss: 0.3045 - acc: 0.8743 - precision_ - ETA: 6s -\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.2578 - acc: 0.8930 - precision_7: 0.8951 - recall_7: 0.8905\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.2011 - acc: 0.9216 - precision_7: 0.9256 - recall_7: 0.9171: 19s - loss: 0.1637 - acc: 0.9439 - precision_7: 0.9455 - recall_7:  - ETA: 18s - loss: 0.1616 - acc: 0.9438 - precision_7: 0.9453 - re - ETA: 17s - loss:\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.1545 - acc: 0.9418 - precision_7: 0.9416 - recall_7: 0.9421\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.1224 - acc: 0.9551 - precision_7: 0.9548 - recall_7: 0.9555\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.1016 - acc: 0.9637 - precision_7: 0.9669 - recall_7: 0.9604: 18s - loss: 0.0701 - acc: 0.9776 - pre - ETA: 15s - loss: 0.0736 - acc: 0.9776 - precision_7: 0.9829 - r - ETA: 14s - loss: 0.0780 - acc: 0.9763 - precisio - ETA: 1\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0910 - acc: 0.9704 - precision_7: 0.9708 - recall_7: 0.97010s - loss: 0.0884 - acc: 0.9717 - precision_7: 0.9724 - reca\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0746 - acc: 0.9741 - precision_7: 0.9744 - recall_7: 0.9737\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0619 - acc: 0.9795 - precision_7: 0.9796 - recall_7: 0.9796: 13s - loss: 0.0661 - acc: 0.9786 - precision_7: 0.9788\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0450 - acc: 0.9876 - precision_7: 0.9878 - recall_7: 0.9874\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0805 - acc: 0.9729 - precision_7: 0.9721 - recall_7: 0.9737\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0659 - acc: 0.9778 - precision_7: 0.9767 - recall_7: 0.9791: 19s - loss: 0.0380 - acc: 0.9918 - preci - ETA: 16s - loss: 0.0357 - acc: 0.9913 - precision_7: 0.9914 - recall_7: 0. - ETA: 1s - loss: 0.0639 - acc: 0.9789 \n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 24s 95ms/step - loss: 0.0547 - acc: 0.9814 - precision_7: 0.9808 - recall_7: 0.9820\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 24s 94ms/step - loss: 0.0377 - acc: 0.9882 - precision_7: 0.9878 - recall_7: 0.98863s - loss: 0.0395 - acc: 0.9879 - precision_7: 0.9868 - recall_7: 0. - ETA: 3s - loss: 0 - ETA: 1s - loss: 0.0382 - acc: 0.9879 - precision_7: 0\n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0418 - acc: 0.9864 - precision_7: 0.9873 - recall_7: 0.9854: 12s - loss: 0.0279 - acc: 0.9918 - precision_7 - ETA: 6s - loss: 0.0376 - acc: 0.9881 - pr - ETA: 1s - loss: 0.0413 - acc: 0.9866 - precision_7:\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 24s 92ms/step - loss: 0.0420 - acc: 0.9853 - precision_7: 0.9859 - recall_7: 0.9847\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 24s 93ms/step - loss: 0.0474 - acc: 0.9832 - precision_7: 0.9830 - recall_7: 0.9835: 18s - loss: 0.0520 - acc: 0.9816 - precision_7: 0.9853 - recall_7 - ETA: 17s - l\n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 22s 87ms/step - loss: 0.0372 - acc: 0.9871 - precision_7: 0.9876 - recall_7: 0.98660s - loss: 0.0362 - acc: 0.9875 - precision_7: 0.9880 - \n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.0481 - acc: 0.9843 - precision_7: 0.9830 - recall_7: 0.9856\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.0423 - acc: 0.9849 - precision_7: 0.9863 - recall_7: 0.9835\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.0417 - acc: 0.9881 - precision_7: 0.9874 - recall_7: 0.9888\n",
      "Epoch 27/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.0239 - acc: 0.9928 - precision_7: 0.9932 - recall_7: 0.9925\n",
      "Epoch 28/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.0306 - acc: 0.9909 - precision_7: 0.9905 - recall_7: 0.9912\n",
      "Epoch 29/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.0296 - acc: 0.9897 - precision_7: 0.9907 - recall_7: 0.9886\n",
      "Epoch 30/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.0552 - acc: 0.9799 - precision_7: 0.9787 - recall_7: 0.9813\n",
      "Epoch 31/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.0549 - acc: 0.9814 - precision_7: 0.9820 - recall_7: 0.9808\n",
      "Epoch 32/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.0404 - acc: 0.9872 - precision_7: 0.9871 - recall_7: 0.9874\n",
      "Epoch 00032: early stopping\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 1.4376 - acc: 0.7599 - precision_7: 0.7754 - recall_7: 0.7257\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 8 ...\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.5390 - acc: 0.7314 - precision_8: 0.7359 - recall_8: 0.7190\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.4653 - acc: 0.7771 - precision_8: 0.7840 - recall_8: 0.7628\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.4309 - acc: 0.8030 - precision_8: 0.8096 - recall_8: 0.7906\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.3853 - acc: 0.8283 - precision_8: 0.8307 - recall_8: 0.8234\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.3556 - acc: 0.8433 - precision_8: 0.8464 - recall_8: 0.8375\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.3085 - acc: 0.8706 - precision_8: 0.8735 - recall_8: 0.8656\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.2535 - acc: 0.8959 - precision_8: 0.8968 - recall_8: 0.8940\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.2125 - acc: 0.9151 - precision_8: 0.9188 - recall_8: 0.9101\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.1573 - acc: 0.9403 - precision_8: 0.9414 - recall_8: 0.9387\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.1274 - acc: 0.9548 - precision_8: 0.9559 - recall_8: 0.9533\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.1032 - acc: 0.9621 - precision_8: 0.9633 - recall_8: 0.9607\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.1034 - acc: 0.9646 - precision_8: 0.9657 - recall_8: 0.9631\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.0716 - acc: 0.9781 - precision_8: 0.9785 - recall_8: 0.9775\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 19s 73ms/step - loss: 0.0486 - acc: 0.9831 - precision_8: 0.9834 - recall_8: 0.9827\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 19s 74ms/step - loss: 0.0741 - acc: 0.9737 - precision_8: 0.9743 - recall_8: 0.9729\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 19s 75ms/step - loss: 0.0673 - acc: 0.9776 - precision_8: 0.9787 - recall_8: 0.9763\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 19s 72ms/step - loss: 0.0496 - acc: 0.9841 - precision_8: 0.9863 - recall_8: 0.9817\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 18s 72ms/step - loss: 0.0450 - acc: 0.9872 - precision_8: 0.9863 - recall_8: 0.9880\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.0366 - acc: 0.9894 - precision_8: 0.9893 - recall_8: 0.9895\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 18s 69ms/step - loss: 0.0391 - acc: 0.9875 - precision_8: 0.9897 - recall_8: 0.9851\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 16s 62ms/step - loss: 0.0613 - acc: 0.9771 - precision_8: 0.9798 - recall_8: 0.9741\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 14s 55ms/step - loss: 0.0258 - acc: 0.9925 - precision_8: 0.9927 - recall_8: 0.9922\n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 18s 70ms/step - loss: 0.0535 - acc: 0.9817 - precision_8: 0.9833 - recall_8: 0.9800\n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.0386 - acc: 0.9866 - precision_8: 0.9870 - recall_8: 0.9861\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.0308 - acc: 0.9905 - precision_8: 0.9898 - recall_8: 0.9912\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.0393 - acc: 0.9866 - precision_8: 0.9866 - recall_8: 0.9866\n",
      "Epoch 27/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.0510 - acc: 0.9839 - precision_8: 0.9841 - recall_8: 0.9836\n",
      "Epoch 00027: early stopping\n",
      "29/29 [==============================] - 1s 25ms/step - loss: 1.2607 - acc: 0.7599 - precision_8: 0.7939 - recall_8: 0.7213\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 9 ...\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.5271 - acc: 0.7426 - precision_9: 0.7531 - recall_9: 0.7185\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 18s 70ms/step - loss: 0.4717 - acc: 0.7805 - precision_9: 0.7916 - recall_9: 0.7589\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 18s 70ms/step - loss: 0.4326 - acc: 0.8008 - precision_9: 0.8123 - recall_9: 0.7801\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 18s 70ms/step - loss: 0.3987 - acc: 0.8198 - precision_9: 0.8292 - recall_9: 0.8036\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 18s 72ms/step - loss: 0.3584 - acc: 0.8421 - precision_9: 0.8449 - recall_9: 0.8364\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.3084 - acc: 0.8677 - precision_9: 0.8730 - recall_9: 0.8591\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 17s 68ms/step - loss: 0.2685 - acc: 0.8888 - precision_9: 0.8933 - recall_9: 0.8821\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 18s 69ms/step - loss: 0.2233 - acc: 0.9084 - precision_9: 0.9094 - recall_9: 0.9063\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.1769 - acc: 0.9305 - precision_9: 0.9318 - recall_9: 0.9283\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.1445 - acc: 0.9486 - precision_9: 0.9480 - recall_9: 0.9489\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.1147 - acc: 0.9576 - precision_9: 0.9559 - recall_9: 0.9592\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 18s 69ms/step - loss: 0.0682 - acc: 0.9780 - precision_9: 0.9775 - recall_9: 0.9782\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 18s 69ms/step - loss: 0.0817 - acc: 0.9735 - precision_9: 0.9736 - recall_9: 0.9731\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.0747 - acc: 0.9738 - precision_9: 0.9729 - recall_9: 0.9746\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.0822 - acc: 0.9692 - precision_9: 0.9687 - recall_9: 0.9694\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 13s 52ms/step - loss: 0.0617 - acc: 0.9809 - precision_9: 0.9809 - recall_9: 0.9807\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 17s 64ms/step - loss: 0.0396 - acc: 0.9877 - precision_9: 0.9871 - recall_9: 0.9883\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 18s 69ms/step - loss: 0.0600 - acc: 0.9813 - precision_9: 0.9807 - recall_9: 0.9817\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.0586 - acc: 0.9808 - precision_9: 0.9814 - recall_9: 0.9799\n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.0359 - acc: 0.9884 - precision_9: 0.9890 - recall_9: 0.9878\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.0273 - acc: 0.9926 - precision_9: 0.9929 - recall_9: 0.9922\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.0366 - acc: 0.9879 - precision_9: 0.9873 - recall_9: 0.9885\n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 18s 70ms/step - loss: 0.0328 - acc: 0.9900 - precision_9: 0.9914 - recall_9: 0.9885\n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 17s 68ms/step - loss: 0.0285 - acc: 0.9916 - precision_9: 0.9929 - recall_9: 0.9902\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 18s 69ms/step - loss: 0.0609 - acc: 0.9797 - precision_9: 0.9785 - recall_9: 0.9807\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 17s 67ms/step - loss: 0.0643 - acc: 0.9780 - precision_9: 0.9810 - recall_9: 0.9746\n",
      "Epoch 00026: early stopping\n",
      "29/29 [==============================] - 1s 26ms/step - loss: 0.9795 - acc: 0.7686 - precision_9: 0.7678 - recall_9: 0.7954\n",
      "------------------------------------------------------------------------\n",
      "Training for fold 10 ...\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 17s 67ms/step - loss: 0.5351 - acc: 0.7327 - precision_10: 0.7361 - recall_10: 0.7278\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 18s 69ms/step - loss: 0.4731 - acc: 0.7793 - precision_10: 0.7853 - recall_10: 0.7706\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.4350 - acc: 0.7992 - precision_10: 0.8131 - recall_10: 0.7786\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 17s 67ms/step - loss: 0.4067 - acc: 0.8143 - precision_10: 0.8295 - recall_10: 0.7927\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 17s 68ms/step - loss: 0.3671 - acc: 0.8369 - precision_10: 0.8497 - recall_10: 0.8196\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 17s 67ms/step - loss: 0.3287 - acc: 0.8587 - precision_10: 0.8696 - recall_10: 0.8449\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.2765 - acc: 0.8828 - precision_10: 0.8898 - recall_10: 0.8745\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 17s 68ms/step - loss: 0.2344 - acc: 0.9066 - precision_10: 0.9110 - recall_10: 0.9019\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 18s 70ms/step - loss: 0.1881 - acc: 0.9270 - precision_10: 0.9307 - recall_10: 0.9230\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 18s 69ms/step - loss: 0.1556 - acc: 0.9419 - precision_10: 0.9441 - recall_10: 0.9398\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 18s 70ms/step - loss: 0.1276 - acc: 0.9536 - precision_10: 0.9597 - recall_10: 0.9473\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 18s 71ms/step - loss: 0.1008 - acc: 0.9649 - precision_10: 0.9669 - recall_10: 0.9631\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 17s 68ms/step - loss: 0.0891 - acc: 0.9686 - precision_10: 0.9671 - recall_10: 0.9704\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 17s 68ms/step - loss: 0.0753 - acc: 0.9742 - precision_10: 0.9733 - recall_10: 0.9752\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 17s 67ms/step - loss: 0.0720 - acc: 0.9760 - precision_10: 0.9755 - recall_10: 0.9767\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 18s 68ms/step - loss: 0.0622 - acc: 0.9785 - precision_10: 0.9796 - recall_10: 0.9774\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 17s 67ms/step - loss: 0.0226 - acc: 0.9948 - precision_10: 0.9961 - recall_10: 0.9934\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 15s 59ms/step - loss: 0.0534 - acc: 0.9834 - precision_10: 0.9847 - recall_10: 0.9823\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 9s 33ms/step - loss: 0.0686 - acc: 0.9777 - precision_10: 0.9802 - recall_10: 0.9752\n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 8s 33ms/step - loss: 0.0404 - acc: 0.9862 - precision_10: 0.9869 - recall_10: 0.9857\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 9s 33ms/step - loss: 0.0543 - acc: 0.9822 - precision_10: 0.9830 - recall_10: 0.9815\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257/257 [==============================] - 9s 33ms/step - loss: 0.0410 - acc: 0.9842 - precision_10: 0.9854 - recall_10: 0.9830\n",
      "Epoch 00022: early stopping\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 1.2613 - acc: 0.7654 - precision_10: 0.7386 - recall_10: 0.8018\n"
     ]
    }
   ],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc,pre,rec,f1=[],[],[],[]\n",
    "pat=5\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=1)\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    X_train=inputs[train].tolist()\n",
    "    y_train=targets[train].tolist()\n",
    "    X_test=inputs[test].tolist()\n",
    "    y_test=targets[test].tolist()\n",
    "    \n",
    "    embedding_layer,X_train,y_train,X_test,y_test= word2vec (X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    model = Sequential() #rnn\n",
    "    model.add(embedding_layer)\n",
    "    model.add(SimpleRNN(128,activation='relu',return_sequences= True))\n",
    "    model.add(SimpleRNN(256,activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                         metrics=['acc',tf.keras.metrics.Precision(),\n",
    "                                  tf.keras.metrics.Recall()]) #binary cross çünkü sonucun pozitif yada negatif\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=50,callbacks=early_stopping)\n",
    "\n",
    "    # evaluate\n",
    "    loss, accuracy, precision, recall = model.evaluate(X_test, y_test)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f1_score)\n",
    "    \n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test Accuracy: 0.764190\n",
      "test precision: 0.768341\n",
      "test recall: 0.759158\n",
      "test f1_score: 0.762515\n"
     ]
    }
   ],
   "source": [
    "print('test Accuracy: %f' % (Average(acc)))\n",
    "print('test precision: %f' % (Average(pre)))\n",
    "print('test recall: %f' % (Average(rec)))\n",
    "print('test f1_score: %f' % (Average(f1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
