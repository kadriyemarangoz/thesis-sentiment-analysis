{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda deactivate\n",
    "!conda activate tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus: \n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=9216)] #7168  6144\n",
    "    )\n",
    "\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#Kütüphanelerin eklenmesi\n",
    "import numpy as np #Bu kütüphane lineer cebir için kullandığımız kütüphane fonksiyonlarını içeriyor\n",
    "import pandas as pd # verilerimizi işlemek için pandas kütüphanasini kullanıyoruz(örn pd.read_scv)\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "import json\n",
    "import random\n",
    "#from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.tr import Turkish\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# DEEP LEARNING IMPORTS\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, MaxPooling2D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer,precision_score,recall_score,f1_score\n",
    "from keras.utils import np_utils\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['tweets','duygu']#,'preprocessing'\n",
    "df = pd.read_excel(\"../dataset/kemik_total.xlsx\")  #kemik_total\n",
    "\n",
    "#column = ['tweets','duygu']\n",
    "#df = pd.read_excel(\"../dataset/kemik_preprocessing.xlsx\")\n",
    "\n",
    "df.columns=column\n",
    "#veri setinin gösterilmesi\n",
    "df=df.drop_duplicates()\n",
    "df['tweets']=df['tweets'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=df['tweets'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tokenizer\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "# Building word indices\n",
    "tokenizer.fit_on_texts(X_t)\n",
    "# Tokenizing sentences\n",
    "sentences = tokenizer.texts_to_sequences(X_t)\n",
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "# Creating texts \n",
    "X_t = list(map(sequence_to_text, sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "n_features=128\n",
    "window_size=5\n",
    "min_count=1\n",
    "epoch=50\n",
    "n_workers=8\n",
    "\n",
    "wordembeddings = Word2Vec(vector_size = n_features,\n",
    "            window = window_size, \n",
    "            min_count= min_count,\n",
    "            workers = n_workers, \n",
    "            sg=1)\n",
    "wordembeddings.build_vocab(X_t)\n",
    "wordembeddings.train(X_t, \n",
    "            total_examples=wordembeddings.corpus_count,  \n",
    "            epochs = epoch)\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=df['tweets'].to_numpy()\n",
    "targets=df['duygu'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec (X_train,y_train,X_test,y_test):\n",
    "    #Create a tokenizer, configured to only take into account the 20 most common words çok küçük olursa kelimeleri \n",
    "    #kaybederiz underfit yaparız\n",
    "    #tokenizer = Tokenizer(lower=True) #en yaygın kaç kelimeyi dikkate alacağı. Belirtilecek en iyi kelime sayısı #1000 yapan da var\n",
    "    tokenizer.fit_on_texts(X_train) #keras tokenizer ile metni dictionary haline getiriyor.\n",
    "    sequences_X_train = tokenizer.texts_to_sequences(X_train) #kelimelerin dictionarydeki karşılığı \n",
    "    #[[2, 1, 3], [2, 1], [4, 1], [5, 6]] şekline getiriliyor. 2-machine 1- learning 3-Knowledge \n",
    "    word_index = tokenizer.word_index #dictionarydeki kelimelerin sayısal karşılığı 'unk': 1, 'ürün': 2,\n",
    "    max_length = 0\n",
    "    for review_number in range(len(sequences_X_train)): #len(sequences_X_train) ile kaç tane [[2,3,4],[2,6]] var bulunuyor burda 2\n",
    "        numberofwords=len(sequences_X_train[review_number]) #[2,3,4] içinde kaç tane şey var 3 burda\n",
    "        if (numberofwords) > (max_length):\n",
    "            max_length = numberofwords #tüm kelimelere bakıp en uzun olanı buluyor\n",
    "\n",
    "    X_train = pad_sequences(sequences_X_train, maxlen=max_length) #ikili boyutlu matrise çevirip her cümelnin uzunluğunu eşit yapıyor.\n",
    "    #En uzun cümle uzunluğuna tamamlanıyor.[[2 1 3] [0 2 1]] alt alta gelecek şekilde en uzun 6 ise 6x6 matris oluyor\n",
    "    y_train = np.asarray(y_train) #tek boyutlu bir matris oluyor [1 1 0 ... 0 1 0] gibi\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "    \n",
    "    sequences_X_test = tokenizer.texts_to_sequences(X_test) #train için yapılan gibi dictionary alınıyor\n",
    "    X_test = pad_sequences(sequences_X_test, maxlen=max_length) #en uzun olana göre pad sequence yapılıyor\n",
    "    y_test = np.asarray(y_test)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "    unique_words = len(word_index) #word_index ile unique olan kelimeler alınıyor 0 dan başladığı için bir arttırılıyor\n",
    "    total_words = unique_words + 1\n",
    "    skipped_words = 0\n",
    "    embedding_dim = n_features #embedding dim vector size ile aynı \n",
    "    embedding_vector=0\n",
    "    embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "    for word, index in tokenizer.word_index.items(): #kelime ve kelimenin dictionarydeki karşılığı alınıyor\n",
    "        try:\n",
    "            embedding_vector = wordembeddings.wv[word]#wordembeddings.word_vectors[wordembeddings.dictionary[word]] #kelimenin word2vec karşılığı vektör olarak\n",
    "\n",
    "        except:\n",
    "            skipped_words = skipped_words+1\n",
    "            pass\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector #dictionarydeki indexine word2vec teki sayısal hali yazılır\n",
    "    embedding_layer = Embedding(total_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "\n",
    "    \n",
    "    return embedding_layer,X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install netron\n",
    "import netron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc,pre,rec,f1,mcc,cohen_kappa=[],[],[],[],[],[]\n",
    "pat = 3\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=True)\n",
    "    \n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    X_train=inputs[train]\n",
    "    y_train=targets[train]\n",
    "    X_test=inputs[test]\n",
    "    y_test=targets[test]\n",
    "\n",
    "    embedding_layer,X_train,y_train,X_test,y_test= word2vec (X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    modelCNN = Sequential() #cnn\n",
    "    modelCNN.add(InputLayer(input_shape=(52,))),\n",
    "    modelCNN.add(embedding_layer)\n",
    "    modelCNN.add(Conv1D(filters=128, kernel_size=3, activation='relu')) #kernal size 5 yan yana beş kelimeye bakması\n",
    "    modelCNN.add(MaxPooling1D(pool_size=3)) #tek satırlık 1d olduğu için\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(64, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(16, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Flatten()) #düzgünleştirmek için\n",
    "    modelCNN.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "    modelCNN.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) #binary cross çünkü sonucun pozitif yada negatif\n",
    "    \n",
    "    #modelCNN.add(Dense(1, activation='sigmoid'))\n",
    "    #modelCNN.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "    \n",
    "    modelCNN.save('model.h5')\n",
    "\n",
    "    netron.start('model.h5')\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    \n",
    "    modelCNN.fit(X_train, y_train, epochs=75,callbacks=early_stopping)\n",
    "\n",
    "    # evaluate\n",
    "    loss, accuracy = modelCNN.evaluate(X_test, y_test)\n",
    "    preds = modelCNN.predict(X_test)\n",
    "    y_true=y_test.argmax(axis=1)\n",
    "    y_pred=preds.argmax(axis=1)\n",
    "    \n",
    "    precision= precision_score(y_true, y_pred, average='weighted')\n",
    "    recall= recall_score(y_true, y_pred, average='weighted')\n",
    "    f1_measure = f1_score(y_true, y_pred, average='weighted')\n",
    "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
    "    c_kappa=cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f1_measure)\n",
    "    mcc.append(mcc_score)\n",
    "    cohen_kappa.append(c_kappa)\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test Accuracy: %f' % (Average(acc)))\n",
    "print('test precision: %f' % (Average(pre)))\n",
    "print('test recall: %f' % (Average(rec)))\n",
    "print('test f1_score: %f' % (Average(f1)))\n",
    "print('test mcc: %f' % (Average(mcc)))\n",
    "print('test cohen_kappa: %f' % (Average(cohen_kappa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kemik data \n",
    "test Accuracy: 0.658395\n",
    "test precision: 0.661111\n",
    "test recall: 0.658395\n",
    "test f1_score: 0.658359\n",
    "test mcc: 0.481480\n",
    "test cohen_kappa: 0.480486\n",
    "    \n",
    "    \n",
    "    \n",
    "#kemik data preprocessing\n",
    "\n",
    "test Accuracy: 0.650157\n",
    "test precision: 0.653050\n",
    "test recall: 0.650157\n",
    "test f1_score: 0.649209\n",
    "test mcc: 0.468766\n",
    "test cohen_kappa: 0.467049\n",
    "    \n",
    "    \n",
    "    \n",
    "    test Accuracy: 0.660702\n",
    "test precision: 0.666117\n",
    "test recall: 0.660702\n",
    "test f1_score: 0.658429\n",
    "test mcc: 0.483724\n",
    "test cohen_kappa: 0.480241"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
