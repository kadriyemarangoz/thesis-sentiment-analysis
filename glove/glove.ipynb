{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#Kütüphanelerin eklenmesi\n",
    "import numpy as np #Bu kütüphane lineer cebir için kullandığımız kütüphane fonksiyonlarını içeriyor\n",
    "import pandas as pd # verilerimizi işlemek için pandas kütüphanasini kullanıyoruz(örn pd.read_scv)\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score\n",
    "!pip install utils\n",
    "from utils import *\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "#from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer,precision_score,recall_score,f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from spacy.lang.tr import Turkish\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# DEEP LEARNING IMPORTS\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, MaxPooling2D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['tweets','duygu'] #,'preprocessing'\n",
    "df = pd.read_excel(\"../dataset/kemik_preprocessing.xlsx\")\n",
    "\n",
    "#column = ['tweets','duygu']\n",
    "#df = pd.read_excel(\"../dataset/kemik_pos_neg.xlsx\")\n",
    "#df = pd.read_excel(\"../dataset/preprocessing_kemik.xlsx\")\n",
    "\n",
    "df.columns=column\n",
    "#veri setinin gösterilmesi\n",
    "df=df.drop_duplicates()\n",
    "df['tweets']=df['tweets'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred)) # print classification report\n",
    "    \n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_precision_score(y_true, y_pred):\n",
    "    \n",
    "    return precision_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_recall_score(y_true, y_pred):\n",
    "    \n",
    "    return recall_score(y_true, y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_f1_score(y_true, y_pred):\n",
    "    \n",
    "    return f1_score(y_true, y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_mcc_score(y_true, y_pred):\n",
    "    \n",
    "    return matthews_corrcoef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_c_kappa(y_true, y_pred):\n",
    "\n",
    "    return cohen_kappa_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = [word_tokenize(i) for i in df.tweets.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the glove library\n",
    "from glove import Corpus, Glove\n",
    "# creating a corpus object\n",
    "corpus = Corpus() \n",
    "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "corpus.fit(X_t, window=10)\n",
    "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
    "#We can set the learning rate as it uses Gradient Descent and number of components\n",
    "glove = Glove(no_components=256, learning_rate=0.1)\n",
    " \n",
    "glove.fit(corpus.matrix, epochs=200, no_threads=8, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "wordembeddings=glove.load('glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def feature_extraction_glove(text):\n",
    "    sentence=\"\"\n",
    "    sentence=str(text) #alınan veriyi string çeviriyoruz\n",
    "    tokens=word_tokenize(sentence)\n",
    "    new_tokens=[token for token in tokens] #stopword listesinde olan kelimeleri çıkarıyoruz\n",
    "    sent_list=[]\n",
    "\n",
    "    for word in new_tokens:\n",
    "        wv=wordembeddings.word_vectors[wordembeddings.dictionary[word]] #glove modeline kelimeyi yolluyorum\n",
    "        sent_list.append(wv)\n",
    " \n",
    "    sent_embed= np.mean(sent_list,axis=0) # fasttext gösterimlerinin ortalamasını alarak cümle vektörünü çıktı olarak veriyor\n",
    "    return sent_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_total= []\n",
    "y_total=df.duygu.tolist()\n",
    "\n",
    "X_total = []\n",
    "for element in tqdm(df.tweets):\n",
    "    X_total.append(feature_extraction_glove(element))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "svc = svm.SVC(kernel='linear')\n",
    "clf = OneVsRestClassifier(svc)\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for word2vec/SVM: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for word2vec/SVM: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "              scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for word2vec/SVM: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for word2vec/SVM: \",f1.mean())\n",
    "\n",
    "mcc_score = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_mcc_score))\n",
    "\n",
    "print(\"MCC_score for word2vec/SVM: \",mcc_score.mean())\n",
    "\n",
    "c_kappa = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_c_kappa))\n",
    "\n",
    "print(\"C_kappa for word2vec/SVM: \",c_kappa.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf =  LogisticRegression(multi_class='ovr')\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for word2vec/LogisticRegression: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for word2vec/LogisticRegression: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for word2vec/LogisticRegression: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for word2vec/LogisticRegression: \",f1.mean())\n",
    "\n",
    "mcc_score = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_mcc_score))\n",
    "\n",
    "print(\"MCC_score for word2vec/LogisticRegression: \",mcc_score.mean())\n",
    "\n",
    "c_kappa = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_c_kappa))\n",
    "\n",
    "print(\"C_kappa for word2vec/LogisticRegression: \",c_kappa.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(10, 3), random_state=1)\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for glove/MLPC: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for glove/MLPC: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for glove/MLPC: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for glove/MLPC: \",f1.mean())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for word2vec/NB: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for word2vec/NB: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for word2vec/NB: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for word2vec/NB: \",f1.mean())\n",
    "\n",
    "mcc_score = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_mcc_score))\n",
    "\n",
    "print(\"MCC_score for word2vec/NB: \",mcc_score.mean())\n",
    "\n",
    "c_kappa = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_c_kappa))\n",
    "\n",
    "print(\"C_kappa for word2vec/NB: \",c_kappa.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
