{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "tf.test.is_gpu_available()\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus: \n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=14336 )] #7168  6144\n",
    "    )\n",
    "\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kütüphanelerin eklenmesi\n",
    "import numpy as np #Bu kütüphane lineer cebir için kullandığımız kütüphane fonksiyonlarını içeriyor\n",
    "import pandas as pd # verilerimizi işlemek için pandas kütüphanasini kullanıyoruz(örn pd.read_scv)\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score\n",
    "!pip install utils\n",
    "from utils import *\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "#from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer,precision_score,recall_score,f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from spacy.lang.tr import Turkish\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# DEEP LEARNING IMPORTS\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, MaxPooling2D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from sklearn.pipeline import Pipeline\n",
    "import time\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['tweets','duygu']\n",
    "df = pd.read_excel(\"../dataset/kemik_preprocessing.xlsx\")\n",
    "\n",
    "#column = ['tweets','duygu']\n",
    "#df = pd.read_excel(\"../dataset/kemik_pos_neg.xlsx\")\n",
    "#df = pd.read_excel(\"../dataset/preprocessing_kemik.xlsx\")\n",
    "\n",
    "df.columns=column\n",
    "#veri setinin gösterilmesi\n",
    "df=df.drop_duplicates()\n",
    "df['tweets']=df['tweets'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=df['tweets'].to_numpy()\n",
    "targets=df['duygu'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t=df['tweets'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing library\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "# Creating a tokenizer\n",
    "tokenizer = Tokenizer(lower=True)\n",
    "# Building word indices\n",
    "tokenizer.fit_on_texts(X_t)\n",
    "# Tokenizing sentences\n",
    "sentences = tokenizer.texts_to_sequences(X_t)\n",
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "# Creating texts \n",
    "X_t = list(map(sequence_to_text, sentences))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install glove-python-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the glove library\n",
    "from glove import Corpus, Glove\n",
    "# creating a corpus object\n",
    "corpus = Corpus() \n",
    "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "corpus.fit(X_t, window=5)\n",
    "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
    "#We can set the learning rate as it uses Gradient Descent and number of components\n",
    "glove = Glove(no_components=256, learning_rate=0.1)\n",
    " \n",
    "glove.fit(corpus.matrix, epochs=200, no_threads=8, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordembeddings=glove.load('glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove (X_train,y_train,X_test,y_test):\n",
    "    #Create a tokenizer, configured to only take into account the 20 most common words çok küçük olursa kelimeleri \n",
    "    #kaybederiz underfit yaparız\n",
    "    #tokenizer = Tokenizer(lower=True) #en yaygın kaç kelimeyi dikkate alacağı. Belirtilecek en iyi kelime sayısı #1000 yapan da var\n",
    "    tokenizer.fit_on_texts(X_train) #keras tokenizer ile metni dictionary haline getiriyor.\n",
    "    sequences_X_train = tokenizer.texts_to_sequences(X_train) #kelimelerin dictionarydeki karşılığı \n",
    "    #[[2, 1, 3], [2, 1], [4, 1], [5, 6]] şekline getiriliyor. 2-machine 1- learning 3-Knowledge \n",
    "    word_index = tokenizer.word_index #dictionarydeki kelimelerin sayısal karşılığı 'unk': 1, 'ürün': 2,\n",
    "    max_length = 0\n",
    "    for review_number in range(len(sequences_X_train)): #len(sequences_X_train) ile kaç tane [[2,3,4],[2,6]] var bulunuyor burda 2\n",
    "        numberofwords=len(sequences_X_train[review_number]) #[2,3,4] içinde kaç tane şey var 3 burda\n",
    "        if (numberofwords) > (max_length):\n",
    "            max_length = numberofwords #tüm kelimelere bakıp en uzun olanı buluyor\n",
    "\n",
    "    X_train = pad_sequences(sequences_X_train, maxlen=max_length) #ikili boyutlu matrise çevirip her cümelnin uzunluğunu eşit yapıyor.\n",
    "    #En uzun cümle uzunluğuna tamamlanıyor.[[2 1 3] [0 2 1]] alt alta gelecek şekilde en uzun 6 ise 6x6 matris oluyor\n",
    "    y_train = np.asarray(y_train) #tek boyutlu bir matris oluyor [1 1 0 ... 0 1 0] gibi\n",
    "    y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "    sequences_X_test = tokenizer.texts_to_sequences(X_test) #train için yapılan gibi dictionary alınıyor\n",
    "    X_test = pad_sequences(sequences_X_test, maxlen=max_length) #en uzun olana göre pad sequence yapılıyor\n",
    "    y_test = np.asarray(y_test)\n",
    "    y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "    unique_words = len(word_index) #word_index ile unique olan kelimeler alınıyor 0 dan başladığı için bir arttırılıyor\n",
    "    total_words = unique_words + 1\n",
    "    skipped_words = 0\n",
    "    embedding_dim = 256 #embedding dim vector size ile aynı \n",
    "    embedding_vector=0\n",
    "    embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "    for word, index in tokenizer.word_index.items(): #kelime ve kelimenin dictionarydeki karşılığı alınıyor\n",
    "        try:\n",
    "            embedding_vector = wordembeddings.word_vectors[wordembeddings.dictionary[word]] #kelimenin word2vec karşılığı vektör olarak\n",
    "        except:\n",
    "            skipped_words = skipped_words+1\n",
    "            pass\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector #dictionarydeki indexine word2vec teki sayısal hali yazılır\n",
    "            \n",
    "    embedding_layer = Embedding(total_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    \n",
    "    return embedding_layer,X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc,pre,rec,f1,mcc,cohen_kappa=[],[],[],[],[],[]\n",
    "pat = 3\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=True)\n",
    "\n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    X_train=inputs[train].tolist()\n",
    "    y_train=targets[train].tolist()\n",
    "    X_test=inputs[test].tolist()\n",
    "    y_test=targets[test].tolist()\n",
    "    \n",
    "    embedding_layer,X_train,y_train,X_test,y_test= glove (X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    model = Sequential() #rnn\n",
    "    model.add(embedding_layer)\n",
    "    model.add(SimpleRNN(256,activation='relu',return_sequences= True))\n",
    "    model.add(SimpleRNN(128,activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) #binary cross çünkü sonucun pozitif yada negatif\n",
    "\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=75,callbacks=early_stopping)\n",
    "\n",
    "    # evaluate\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    preds = model.predict(X_test)\n",
    "    y_true=y_test.argmax(axis=1)\n",
    "    y_pred=preds.argmax(axis=1)\n",
    "    \n",
    "    precision= precision_score(y_true, y_pred, average='weighted')\n",
    "    recall= recall_score(y_true, y_pred, average='weighted')\n",
    "    f1_measure = f1_score(y_true, y_pred, average='weighted')\n",
    "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
    "    c_kappa=cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f1_measure)\n",
    "    mcc.append(mcc_score)\n",
    "    cohen_kappa.append(c_kappa)\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test Accuracy: %f' % (Average(acc)))\n",
    "print('test precision: %f' % (Average(pre)))\n",
    "print('test recall: %f' % (Average(rec)))\n",
    "print('test f1_score: %f' % (Average(f1)))\n",
    "print('test mcc: %f' % (Average(mcc)))\n",
    "print('test cohen_kappa: %f' % (Average(cohen_kappa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc,pre,rec,f1,mcc,cohen_kappa=[],[],[],[],[],[]\n",
    "pat = 3\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=True)\n",
    "    \n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    X_train=inputs[train]\n",
    "    y_train=targets[train]\n",
    "    X_test=inputs[test]\n",
    "    y_test=targets[test]\n",
    "\n",
    "    embedding_layer,X_train,y_train,X_test,y_test= glove (X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    modelCNN = Sequential() #cnn\n",
    "\n",
    "    modelCNN.add(embedding_layer)\n",
    "    modelCNN.add(Conv1D(filters=128, kernel_size=3, activation='relu')) #kernal size 5 yan yana beş kelimeye bakması\n",
    "    modelCNN.add(MaxPooling1D(pool_size=3)) #tek satırlık 1d olduğu için\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(64, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(16, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Flatten()) #düzgünleştirmek için\n",
    "    modelCNN.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "    modelCNN.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) #binary cross çünkü sonucun pozitif yada negatif\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    \n",
    "    modelCNN.fit(X_train, y_train, epochs=75,callbacks=early_stopping)\n",
    "\n",
    "    # evaluate\n",
    "    loss, accuracy = modelCNN.evaluate(X_test, y_test)\n",
    "    preds = modelCNN.predict(X_test)\n",
    "    y_true=y_test.argmax(axis=1)\n",
    "    y_pred=preds.argmax(axis=1)\n",
    "    \n",
    "    precision= precision_score(y_true, y_pred, average='weighted')\n",
    "    recall= recall_score(y_true, y_pred, average='weighted')\n",
    "    f1_measure = f1_score(y_true, y_pred, average='weighted')\n",
    "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
    "    c_kappa=cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f1_measure)\n",
    "    mcc.append(mcc_score)\n",
    "    cohen_kappa.append(c_kappa)\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test Accuracy: %f' % (Average(acc)))\n",
    "print('test precision: %f' % (Average(pre)))\n",
    "print('test recall: %f' % (Average(rec)))\n",
    "print('test f1_score: %f' % (Average(f1)))\n",
    "print('test mcc: %f' % (Average(mcc)))\n",
    "print('test cohen_kappa: %f' % (Average(cohen_kappa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc,pre,rec,f1,mcc,cohen_kappa=[],[],[],[],[],[]\n",
    "pat = 3\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=True)\n",
    "    \n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    X_train=inputs[train]\n",
    "    y_train=targets[train]\n",
    "    X_test=inputs[test]\n",
    "    y_test=targets[test]\n",
    "\n",
    "    embedding_layer,X_train,y_train,X_test,y_test= glove (X_train,y_train,X_test,y_test)\n",
    "\n",
    "    modelLSTM = Sequential()\n",
    "    modelLSTM.add(embedding_layer)\n",
    "    modelLSTM.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    modelLSTM.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "    modelLSTM.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) #binary cross çünkü sonucun pozitif yada negatif\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    \n",
    "    modelLSTM.fit(X_train, y_train, epochs=75,callbacks=early_stopping)\n",
    "\n",
    "    # evaluate\n",
    "    loss, accuracy = modelLSTM.evaluate(X_test, y_test)\n",
    "    preds = modelLSTM.predict(X_test)\n",
    "    y_true=y_test.argmax(axis=1)\n",
    "    y_pred=preds.argmax(axis=1)\n",
    "    \n",
    "    precision= precision_score(y_true, y_pred, average='weighted')\n",
    "    recall= recall_score(y_true, y_pred, average='weighted')\n",
    "    f1_measure = f1_score(y_true, y_pred, average='weighted')\n",
    "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
    "    c_kappa=cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f1_measure)\n",
    "    mcc.append(mcc_score)\n",
    "    cohen_kappa.append(c_kappa)\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test Accuracy: %f' % (Average(acc)))\n",
    "print('test precision: %f' % (Average(pre)))\n",
    "print('test recall: %f' % (Average(rec)))\n",
    "print('test f1_score: %f' % (Average(f1)))\n",
    "print('test mcc: %f' % (Average(mcc)))\n",
    "print('test cohen_kappa: %f' % (Average(cohen_kappa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn+lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc,pre,rec,f1,mcc,cohen_kappa=[],[],[],[],[],[]\n",
    "pat = 3\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=True)\n",
    "    \n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    X_train=inputs[train]\n",
    "    y_train=targets[train]\n",
    "    X_test=inputs[test]\n",
    "    y_test=targets[test]\n",
    "\n",
    "    embedding_layer,X_train,y_train,X_test,y_test= glove (X_train,y_train,X_test,y_test)\n",
    "\n",
    "    modelCNNLSTM = Sequential() #cnn\n",
    "\n",
    "    modelCNNLSTM.add(embedding_layer)\n",
    "    modelCNNLSTM.add(Conv1D(filters=128, kernel_size=3, activation='relu')) #kernal size 5 yan yana beş kelimeye bakması\n",
    "    modelCNNLSTM.add(MaxPooling1D(pool_size=3)) #tek satırlık 1d olduğu için\n",
    "    modelCNNLSTM.add(Dropout(0.3))\n",
    "    modelCNNLSTM.add(Dense(64, activation='relu'))\n",
    "    modelCNNLSTM.add(Dropout(0.3))\n",
    "    modelCNNLSTM.add(Dense(16, activation='relu'))\n",
    "    modelCNNLSTM.add(Dropout(0.3))\n",
    "    modelCNNLSTM.add(LSTM(128))\n",
    "    modelCNNLSTM.add(Flatten()) #düzgünleştirmek için\n",
    "    modelCNNLSTM.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "    modelCNNLSTM.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy']) #binary cross çünkü sonucun pozitif yada negatif\n",
    "    \n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    \n",
    "    modelCNNLSTM.fit(X_train, y_train, epochs=75,callbacks=early_stopping)\n",
    "\n",
    "    # evaluate\n",
    "    loss, accuracy = modelCNNLSTM.evaluate(X_test, y_test)\n",
    "    preds = modelCNNLSTM.predict(X_test)\n",
    "    y_true=y_test.argmax(axis=1)\n",
    "    y_pred=preds.argmax(axis=1)\n",
    "    \n",
    "    precision= precision_score(y_true, y_pred, average='weighted')\n",
    "    recall= recall_score(y_true, y_pred, average='weighted')\n",
    "    f1_measure = f1_score(y_true, y_pred, average='weighted')\n",
    "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
    "    c_kappa=cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f1_measure)\n",
    "    mcc.append(mcc_score)\n",
    "    cohen_kappa.append(c_kappa)\n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test Accuracy: %f' % (Average(acc)))\n",
    "print('test precision: %f' % (Average(pre)))\n",
    "print('test recall: %f' % (Average(rec)))\n",
    "print('test f1_score: %f' % (Average(f1)))\n",
    "print('test mcc: %f' % (Average(mcc)))\n",
    "print('test cohen_kappa: %f' % (Average(cohen_kappa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
