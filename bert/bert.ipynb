{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kütüphanelerin eklenmesi\n",
    "import numpy as np #Bu kütüphane lineer cebir için kullandığımız kütüphane fonksiyonlarını içeriyor\n",
    "import pandas as pd # verilerimizi işlemek için pandas kütüphanasini kullanıyoruz(örn pd.read_scv)\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score\n",
    "!pip install utils\n",
    "from utils import *\n",
    "import random\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "warnings.filterwarnings('ignore') \n",
    "import string\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import xlsxwriter \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "import time\n",
    "from spacy.lang.tr import Turkish\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer,precision_score,recall_score,f1_score\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from glove import Corpus, Glove\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import nltk \n",
    "import torch\n",
    "\n",
    "import math\n",
    "import bert\n",
    "from tensorflow.keras import layers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['tweets','duygu']\n",
    "df = pd.read_excel(\"../dataset/kemik_pos_neg.xlsx\")\n",
    "df.columns=column\n",
    "#veri setinin gösterilmesi\n",
    "df=df.drop_duplicates()\n",
    "df['tweets']=df['tweets'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred)) # print classification report\n",
    "    \n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_precision_score(y_true, y_pred):\n",
    "    \n",
    "    return precision_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_recall_score(y_true, y_pred):\n",
    "    \n",
    "    return recall_score(y_true, y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_f1_score(y_true, y_pred):\n",
    "    \n",
    "    return f1_score(y_true, y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pre = AutoModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    " \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\",do_lower_case=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_text_preparation(text, tokenizer):\n",
    "    \"\"\"Preparing the input for BERT\n",
    "    \n",
    "    Takes a string argument and performs\n",
    "    pre-processing like adding special tokens,\n",
    "    tokenization, tokens to ids, and tokens to\n",
    "    segment ids. All tokens are mapped to seg-\n",
    "    ment id = 1.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Text to be converted\n",
    "        tokenizer (obj): Tokenizer object\n",
    "            to convert text into BERT-re-\n",
    "            adable tokens and ids\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BERT-readable tokens\n",
    "        obj: Torch tensor with token ids\n",
    "        obj: Torch tensor segment ids\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    " \n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "                        text,                     \n",
    "                        add_special_tokens = True,\n",
    "                        max_length = 512,      \n",
    "                        padding = True,\n",
    "                        return_attention_mask = True,\n",
    "                        truncation=True,\n",
    "                        return_tensors = 'pt',\n",
    "                   )\n",
    "    \n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    " \n",
    "    tokens_tensor = torch.cat(input_ids, dim=0)\n",
    "    segments_tensors = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    return tokens_tensor, segments_tensors\n",
    "    \n",
    "def get_bert_embeddings(tokens_tensor, segments_tensors, model_pre):\n",
    "    \"\"\"Get embeddings from an embedding model\n",
    "    \n",
    "    Args:\n",
    "        tokens_tensor (obj): Torch tensor size [n_tokens]\n",
    "            with token ids for each token in text\n",
    "        segments_tensors (obj): Torch tensor size [n_tokens]\n",
    "            with segment ids for each token in text\n",
    "        model (obj): Embedding model to generate embeddings\n",
    "            from token and segment ids\n",
    "    \n",
    "    Returns:\n",
    "        list: List of list of floats of size\n",
    "            [n_tokens, n_embedding_dimensions]\n",
    "            containing embeddings for each token\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Gradient calculation id disabled\n",
    "    # Model is in inference mode\n",
    "    with torch.no_grad():\n",
    "        outputs = model_pre(tokens_tensor, segments_tensors)\n",
    "        #print(\"outputs\",outputs)\n",
    "        # Removing the first hidden state\n",
    "       # The first state is the input state\n",
    "        hidden_states = outputs[1]\n",
    " \n",
    "    token_vecs = hidden_states\n",
    "    # Calculate the average of all ? token vectors.\n",
    "    sentence_embedding = torch.mean(token_vecs, dim=0)\n",
    "    # Move to the CPU and convert to numpy ndarray.\n",
    "    sentence_embedding = sentence_embedding.detach().cpu().numpy()\n",
    " \n",
    "    return np.array(sentence_embedding.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_bert(text):\n",
    "    tokens_tensor, segments_tensors = bert_text_preparation(text, tokenizer)\n",
    "    sentence_embedding = get_bert_embeddings(tokens_tensor, segments_tensors, model_pre)\n",
    "        \n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_total= []\n",
    "y_total=df.duygu.tolist()\n",
    "\n",
    "X_total = []\n",
    "for element in tqdm(df.tweets):\n",
    "    X_total.append(feature_extraction_bert(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for bert/SVM: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for bert/SVM: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for bert/SVM: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for bert/SVM: \",f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "clf = LogisticRegression()\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for bert/LogisticRegression: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for bert/LogisticRegression: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for bert/LogisticRegression: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for bert/LogisticRegression: \",f1.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(10, 3), random_state=1)\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for bert/MLPC: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for bert/MLPC: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for bert/MLPC: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for bert/MLPC: \",f1.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
