{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib \n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "tf.test.is_gpu_available()\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus: \n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=14336 )] #7168  6144\n",
    "    )\n",
    "\n",
    "logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kütüphanelerin eklenmesi\n",
    "import locale\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
    "import numpy as np #Bu kütüphane lineer cebir için kullandığımız kütüphane fonksiyonlarını içeriyor\n",
    "import pandas as pd # verilerimizi işlemek için pandas kütüphanasini kullanıyoruz(örn pd.read_scv)\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import KFold, cross_val_predict, cross_val_score\n",
    "!pip install utils\n",
    "from utils import *\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "#from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer,precision_score,recall_score,f1_score\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from spacy.lang.tr import Turkish\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "!pip install transformers\n",
    "!pip install bert-tensorflow\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import nltk \n",
    "import torch\n",
    "import tqdm\n",
    "import math\n",
    "import bert\n",
    "from tensorflow.keras import layers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import re\n",
    "import random\n",
    "\n",
    "# DEEP LEARNING IMPORTS\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Activation, Dropout, Flatten, MaxPooling2D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import math\n",
    "import bert\n",
    "from tensorflow.keras import layers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['tweets','duygu']\n",
    "df = pd.read_excel(\"../dataset/kemik_total.xlsx\")\n",
    "\n",
    "df.columns=column\n",
    "#veri setinin gösterilmesi\n",
    "df=df.drop_duplicates()\n",
    "df['tweets']=df['tweets'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = list(df['tweets'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\",do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 50\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rnn\n",
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=256,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=3,\n",
    "                 dropout_rate=0.3,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.rnn_layer1 = layers.SimpleRNN(128,\n",
    "                                        return_sequences= True,\n",
    "                                        activation=\"relu\")\n",
    "        self.rnn_layer2 = layers.SimpleRNN(256,\n",
    "                                        return_sequences= True,\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.rnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.rnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        \n",
    "        concatenated = tf.concat([l_1,l_2], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc,pre,rec,f1,mcc,cohen_kappa=[],[],[],[],[],[]\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "for train_index, test_index in kfold.split(df):\n",
    "    # splitting Dataframe (dataset not included)\n",
    "    \n",
    "    train_df = df.iloc[train_index].tweets.tolist()\n",
    "    test_df = df.iloc[test_index].tweets.tolist()\n",
    "    train_y = np.array(df.iloc[train_index].duygu.tolist())\n",
    "    test_y = np.array(df.iloc[test_index].duygu.tolist())\n",
    "\n",
    "    tokenized_reviews_train = [tokenize_reviews(tweet) for tweet in train_df]\n",
    "    tokenized_reviews_test = [tokenize_reviews(tweet) for tweet in test_df] \n",
    "    \n",
    "    reviews_with_len_train = [[review, train_y[i], len(review)]\n",
    "                     for i, review in enumerate(tokenized_reviews_train)]\n",
    "    \n",
    "    reviews_with_len_test = [[review, test_y[i], len(review)]\n",
    "                     for i, review in enumerate(tokenized_reviews_test)]\n",
    "\n",
    "    random.shuffle(reviews_with_len_train)\n",
    "    random.shuffle(reviews_with_len_test)\n",
    "    \n",
    "    reviews_with_len_train.sort(key=lambda x: x[2])\n",
    "    reviews_with_len_test.sort(key=lambda x: x[2])\n",
    "    \n",
    "    sorted_reviews_train = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len_train]\n",
    "    sorted_reviews_test = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len_test]\n",
    "\n",
    "    y_true = [review_lab[1] for review_lab in reviews_with_len_test]\n",
    "    \n",
    "    processed_dataset_test = tf.data.Dataset.from_generator(lambda: sorted_reviews_test, output_types=(tf.int32, tf.int32))\n",
    "    processed_dataset_train = tf.data.Dataset.from_generator(lambda: sorted_reviews_train, output_types=(tf.int32, tf.int32))\n",
    "\n",
    "    test_data = processed_dataset_test.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "    train_data = processed_dataset_train.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "    \n",
    "    \"\"\" TOTAL_BATCHES = len(sorted_reviews_train)\n",
    "    TEST_BATCHES = len(sorted_reviews_test)\n",
    "    batched_dataset_train.shuffle(TOTAL_BATCHES)\n",
    "    batched_dataset_test.shuffle(TEST_BATCHES)\n",
    "    test_data = batched_dataset_test.take(TEST_BATCHES)\n",
    "    train_data = batched_dataset_train.skip(TEST_BATCHES)\"\"\"\n",
    "\n",
    "    text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.05)\n",
    "\n",
    "    if OUTPUT_CLASSES == 2:\n",
    "        text_model.compile(loss=\"binary_crossentropy\",\n",
    "                          optimizer=\"adam\",\n",
    "                          metrics=[\"accuracy\"])\n",
    "    else:\n",
    "        text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                          optimizer=opt,\n",
    "                          metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "\n",
    "    text_model.fit(train_data, epochs=NB_EPOCHS,callbacks=early_stopping)\n",
    "    \n",
    "    # evaluate\n",
    "    loss, accuracy = text_model.evaluate(test_data)\n",
    "    preds = text_model.predict(test_data)\n",
    "    y_pred=preds.argmax(axis=1)\n",
    "    \n",
    "    precision= precision_score(y_true, y_pred, average='weighted')\n",
    "    recall= recall_score(y_true, y_pred, average='weighted')\n",
    "    f1_measure = f1_score(y_true, y_pred, average='weighted')\n",
    "    mcc_score = matthews_corrcoef(y_true, y_pred)\n",
    "    c_kappa=cohen_kappa_score(y_true, y_pred)\n",
    "    \n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f1_measure)\n",
    "    mcc.append(mcc_score)\n",
    "    cohen_kappa.append(c_kappa)\n",
    "\n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test Accuracy: %f' % (Average(acc)))\n",
    "print('test precision: %f' % (Average(pre)))\n",
    "print('test recall: %f' % (Average(rec)))\n",
    "print('test f1_score: %f' % (Average(f1)))\n",
    "print('test mcc: %f' % (Average(mcc)))\n",
    "print('test cohen_kappa: %f' % (Average(cohen_kappa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
