{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#Kütüphanelerin eklenmesi\n",
    "import numpy as np #Bu kütüphane lineer cebir için kullandığımız kütüphane fonksiyonlarını içeriyor\n",
    "import pandas as pd # verilerimizi işlemek için pandas kütüphanasini kullanıyoruz(örn pd.read_scv)\n",
    "from sklearn.metrics import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, train_test_split, cross_val_predict, cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "!pip install utils\n",
    "!pip install zemberek-python\n",
    "from utils import *\n",
    "import json\n",
    "import random\n",
    "#from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.tr import Turkish\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "warnings.filterwarnings('ignore') \n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# DEEP LEARNING IMPORTS\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv1D, Activation, Dropout, Flatten, MaxPooling1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from glove import Corpus, Glove\n",
    "\n",
    "#https://medium.com/analytics-vidhya/word-vectorization-using-glove-76919685ee0b\n",
    "#https://github.com/maciejkula/glove-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fasttext modelini git üzerinden çekiyoruz\n",
    "\n",
    "##NOT: JUPYTER C DİSKİ ÜZERİNDE ÇALIŞTIĞI VE DİSKİN DOLU OLMASINDAN DOLAYI VE FASTTEXT\n",
    "#MODELİNİN KAPLADIĞI ALAN YÜZÜNDEN KODU GOOGLE COLAB ÜZERİNDE ÇALIŞTIRDIM O YÜZDEN DOSYA İÇİNDE \n",
    "#FASTTEXT.GİT KLASÖRÜ OLMAYACAKTIR. KODLAR COLAB ÜZERİNDEN İSTENİLİRSE DERLENEBİLİR\n",
    "\n",
    "#!git clone https://github.com/facebookresearch/fastText.git\n",
    "\n",
    "#!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "#Türkçe için önceden eğitilmiş kelime vektörlerini alıyoruz\n",
    "#fasttext.util.download_model('tr', if_exists='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modeli yüklüyoruz\n",
    "ft = fasttext.load_model('D:/Desktop/İndirilenler yedek/cc.tr.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['tweets','duygu','preprocessing']\n",
    "df = pd.read_excel(\"../dataset/total.xlsx\")\n",
    "df.columns=column\n",
    "#veri setinin gösterilmesi\n",
    "df=df.drop_duplicates()\n",
    "df['tweets']=df['tweets'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred)) # print classification report\n",
    "    \n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_precision_score(y_true, y_pred):\n",
    "    \n",
    "    return precision_score(y_true, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_with_accuracy_score(y_true, y_pred):\n",
    "    print(classification_report(y_true, y_pred)) # print classification report\n",
    "    return accuracy_score(y_true, y_pred) # return accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classification_report_with_recall_score(y_true, y_pred):\n",
    "    \n",
    "    return recall_score(y_true, y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classification_report_with_f1_score(y_true, y_pred):\n",
    "    \n",
    "    return f1_score(y_true, y_pred,average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WPT = nltk.WordPunctTokenizer()\n",
    "#verimize preprocessing aşamalarını uyguluyoruz\n",
    "def feature_extraction_fasttext(text):\n",
    "    sentence=\"\"\n",
    "    sentence=str(text) #alınan veriyi string çeviriyoruz\n",
    "    #stop_word_list = nltk.corpus.stopwords.words('turkish') #türkçe için çok tekrar eden kelimeleri (stopwords) cümlelerden çıkarıyoruz\n",
    "    tokens=WPT.tokenize(sentence) #cümleyi kelimelere bölüyoruz\n",
    "    new_tokens=[token for token in tokens ]#if token not in stop_word_list] #stopword listesinde olan kelimeleri çıkarıyoruz\n",
    "    sent_list=[]\n",
    "\n",
    "    for word in new_tokens:\n",
    "        wv=ft.get_word_vector(word) #fasttext modeline kelimeyi yolluyorum\n",
    "        sent_list.append(wv)\n",
    " \n",
    "    sent_embed= np.mean(sent_list,axis=0) # fasttext gösterimlerinin ortalamasını alarak cümle vektörünü çıktı olarak veriyor\n",
    "    return sent_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_total= []\n",
    "y_total=df.duygu.tolist()\n",
    "\n",
    "X_total = []\n",
    "for element in tqdm(df.tweets):\n",
    "    X_total.append(feature_extraction_fasttext(element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for fasttext/SVM: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for fasttext/SVM: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for fasttext/SVM: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for fasttext/SVM: \",f1.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested CV with parameter optimization\n",
    "clf = LogisticRegression()\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for fasttext/LogisticRegression: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for fasttext/LogisticRegression: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for fasttext/LogisticRegression: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for fasttext/LogisticRegression: \",f1.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(10, 3), random_state=1)\n",
    "\n",
    "accuracy = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_accuracy_score))\n",
    "print(\"Accurancy for fasttext/MLPC: \",accuracy.mean())\n",
    "\n",
    "\n",
    "precision = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_precision_score))\n",
    "print(\"Precision for fasttext/MLPC: \",precision.mean())\n",
    "\n",
    "\n",
    "recall = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_recall_score))\n",
    "print(\"Recall for fasttext/MLPC: \",recall.mean())\n",
    "\n",
    "\n",
    "f1 = cross_val_score(clf, X_total, y_total, cv=10, \\\n",
    "               scoring=make_scorer(classification_report_with_f1_score))\n",
    "print(\"F1-Score for fasttext/MLPC: \",f1.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
