{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "#Kütüphanelerin eklenmesi\n",
    "import numpy as np #Bu kütüphane lineer cebir için kullandığımız kütüphane fonksiyonlarını içeriyor\n",
    "import pandas as pd # verilerimizi işlemek için pandas kütüphanasini kullanıyoruz(örn pd.read_scv)\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import *\n",
    "import json\n",
    "import random\n",
    "#from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.tr import Turkish\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "# DEEP LEARNING IMPORTS\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Conv2D, Activation, Dropout, Flatten, MaxPooling2D,Bidirectional\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#column = ['tweets','duygu','preprocessing']\n",
    "#df = pd.read_excel(\"../dataset/total.xlsx\")\n",
    "\n",
    "column = ['tweets','duygu']\n",
    "df = pd.read_excel(\"../dataset/kemik_pos_neg.xlsx\")\n",
    "\n",
    "\n",
    "df.columns=column\n",
    "#veri setinin gösterilmesi\n",
    "df=df.drop_duplicates()\n",
    "df['tweets']=df['tweets'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.duygu==\"olumlu\",\"duygu\"]=1\n",
    "df.loc[df.duygu==\"olumsuz\",\"duygu\"]=0\n",
    "df[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sample(frac=1).reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=df['tweets'].to_numpy()\n",
    "targets=df['duygu'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf (X_train,y_train,X_test,y_test):\n",
    "    #Create a tokenizer, configured to only take into account the 20 most common words çok küçük olursa kelimeleri \n",
    "    #kaybederiz underfit yaparız\n",
    "    tokenizer = Tokenizer(num_words=1000) #en yaygın kaç kelimeyi dikkate alacağı. Belirtilecek en iyi kelime sayısı #1000 yapan da var\n",
    "    tokenizer.fit_on_texts(X_train) #keras tokenizer ile metni dictionary haline getiriyor.\n",
    "    sequences_X_train = tokenizer.texts_to_sequences(X_train) #kelimelerin dictionarydeki karşılığı \n",
    "    #[[2, 1, 3], [2, 1], [4, 1], [5, 6]] şekline getiriliyor. 2-machine 1- learning 3-Knowledge \n",
    "    word_index = tokenizer.word_index #dictionarydeki kelimelerin sayısal karşılığı 'unk': 1, 'ürün': 2,\n",
    "    max_length = 0\n",
    "    for review_number in range(len(sequences_X_train)): #len(sequences_X_train) ile kaç tane [[2,3,4],[2,6]] var bulunuyor burda 2\n",
    "        numberofwords=len(sequences_X_train[review_number]) #[2,3,4] içinde kaç tane şey var 3 burda\n",
    "        if (numberofwords) > (max_length):\n",
    "            max_length = numberofwords #tüm kelimelere bakıp en uzun olanı buluyor\n",
    "\n",
    "    vocabulary=[]\n",
    "    for key in word_index.keys():\n",
    "        vocabulary.append(key)\n",
    "\n",
    "    pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),\n",
    "                     ('tfid', TfidfTransformer())]).fit(X_train)\n",
    "\n",
    "    res = dict(zip(vocabulary, pipe['tfid'].idf_))\n",
    "\n",
    "    X_train = pad_sequences(sequences_X_train, maxlen=max_length) #ikili boyutlu matrise çevirip her cümelnin uzunluğunu eşit yapıyor.\n",
    "    #En uzun cümle uzunluğuna tamamlanıyor.[[2 1 3] [0 2 1]] alt alta gelecek şekilde en uzun 6 ise 6x6 matris oluyor\n",
    "    y_train = np.asarray(y_train) #tek boyutlu bir matris oluyor [1 1 0 ... 0 1 0] gibi\n",
    "\n",
    "    sequences_X_test = tokenizer.texts_to_sequences(X_test) #train için yapılan gibi dictionary alınıyor\n",
    "    X_test = pad_sequences(sequences_X_test, maxlen=max_length) #en uzun olana göre pad sequence yapılıyor\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "\n",
    "    unique_words = len(word_index) #word_index ile unique olan kelimeler alınıyor 0 dan başladığı için bir arttırılıyor\n",
    "    total_words = unique_words + 1\n",
    "    \n",
    "    skipped_words = 0\n",
    "    embedding_dim = 1 #embedding dim vector size ile aynı \n",
    "    embedding_vector=0\n",
    "    embedding_matrix = np.zeros((total_words, embedding_dim))\n",
    "    for word, index in tokenizer.word_index.items(): #kelime ve kelimenin dictionarydeki karşılığı alınıyor\n",
    "        try:\n",
    "            embedding_vector = res[word] #kelimenin word2vec karşılığı vektör olarak\n",
    "        except:\n",
    "            skipped_words = skipped_words+1\n",
    "            pass\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector \n",
    "            \n",
    "    embedding_layer = Embedding(total_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "\n",
    "    return embedding_layer,X_train,y_train,X_test,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 373, 1)            7044      \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 369, 256)          1536      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 184, 256)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 184, 256)          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 184, 360)          92520     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 184, 360)          0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 184, 300)          108300    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 184, 300)          0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 184, 260)          78260     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 184, 260)          0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 184, 150)          39150     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 184, 150)          0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 184, 120)          18120     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 184, 120)          0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 184, 80)           9680      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 184, 80)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 14720)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 14721     \n",
      "=================================================================\n",
      "Total params: 369,331\n",
      "Trainable params: 362,287\n",
      "Non-trainable params: 7,044\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Define the K-fold Cross Validator\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "# K-fold Cross Validation model evaluation\n",
    "fold_no = 1\n",
    "acc,pre,rec,f1=[],[],[],[]\n",
    "pat = 5\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=True)\n",
    "    \n",
    "for train, test in kfold.split(inputs, targets):\n",
    "    \n",
    "    X_train=inputs[train].tolist()\n",
    "    y_train=targets[train].tolist()\n",
    "    X_test=inputs[test].tolist()\n",
    "    y_test=targets[test].tolist()\n",
    "    \n",
    "    embedding_layer,X_train,y_train,X_test,y_test= tf_idf (X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    modelCNN = Sequential() #cnn\n",
    "\n",
    "    modelCNN.add(embedding_layer)\n",
    "    modelCNN.add(Conv1D(filters=256, kernel_size=5, activation='relu')) #kernal size 5 yan yana beş kelimeye bakması\n",
    "    modelCNN.add(MaxPooling1D(pool_size=2)) #tek satırlık 1d olduğu için\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(360, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(300, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(260, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(150, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(120, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Dense(80, activation='relu'))\n",
    "    modelCNN.add(Dropout(0.3))\n",
    "    modelCNN.add(Flatten()) #düzgünleştirmek için\n",
    "    modelCNN.add(Dense(1, activation='sigmoid'))\n",
    "    modelCNN.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "                         metrics=['acc',tf.keras.metrics.Precision(),\n",
    "                                  tf.keras.metrics.Recall()]) #binary cross çünkü sonucun pozitif yada negatif\n",
    "\n",
    "    # Generate a print\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Training for fold {fold_no} ...')\n",
    "    \n",
    "    \n",
    "    modelCNN.fit(X_train, y_train, epochs=50,callbacks=early_stopping)\n",
    "\n",
    "    # evaluate\n",
    "    loss, accuracy, precision, recall = modelCNN.evaluate(X_test, y_test)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    acc.append(accuracy)\n",
    "    pre.append(precision)\n",
    "    rec.append(recall)\n",
    "    f1.append(f1_score)\n",
    "    \n",
    "    \n",
    "    # Increase fold number\n",
    "    fold_no = fold_no + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "420/420 [==============================] - 132s 314ms/step - loss: 0.6727 - accuracy: 0.5895\n",
      "Epoch 2/100\n",
      "420/420 [==============================] - 133s 316ms/step - loss: 0.6749 - accuracy: 0.5874\n",
      "Epoch 3/100\n",
      "420/420 [==============================] - 161s 383ms/step - loss: 0.6652 - accuracy: 0.6064\n",
      "Epoch 4/100\n",
      "420/420 [==============================] - 231s 551ms/step - loss: 0.6653 - accuracy: 0.6043\n",
      "Epoch 5/100\n",
      "420/420 [==============================] - 248s 591ms/step - loss: 0.6620 - accuracy: 0.6075\n",
      "Epoch 6/100\n",
      "420/420 [==============================] - 218s 520ms/step - loss: 0.6595 - accuracy: 0.6082\n",
      "Epoch 7/100\n",
      "420/420 [==============================] - 197s 469ms/step - loss: 0.6585 - accuracy: 0.6115\n",
      "Epoch 8/100\n",
      "420/420 [==============================] - 196s 467ms/step - loss: 0.6575 - accuracy: 0.6172\n",
      "Epoch 9/100\n",
      "420/420 [==============================] - 208s 496ms/step - loss: 0.6561 - accuracy: 0.6150\n",
      "Epoch 10/100\n",
      "420/420 [==============================] - 215s 513ms/step - loss: 0.6553 - accuracy: 0.6098\n",
      "Epoch 11/100\n",
      "420/420 [==============================] - 204s 486ms/step - loss: 0.6515 - accuracy: 0.6233\n",
      "Epoch 12/100\n",
      "420/420 [==============================] - 248s 591ms/step - loss: 0.6538 - accuracy: 0.6182\n",
      "Epoch 13/100\n",
      "420/420 [==============================] - 242s 576ms/step - loss: 0.6508 - accuracy: 0.6208\n",
      "Epoch 14/100\n",
      "420/420 [==============================] - 248s 590ms/step - loss: 0.6530 - accuracy: 0.6175\n",
      "Epoch 15/100\n",
      "420/420 [==============================] - 253s 603ms/step - loss: 0.6495 - accuracy: 0.6195\n",
      "Epoch 16/100\n",
      "420/420 [==============================] - 245s 584ms/step - loss: 0.6507 - accuracy: 0.6209\n",
      "Epoch 17/100\n",
      "420/420 [==============================] - 240s 573ms/step - loss: 0.6510 - accuracy: 0.6188\n",
      "Epoch 18/100\n",
      "420/420 [==============================] - 246s 585ms/step - loss: 0.6499 - accuracy: 0.6226\n",
      "Epoch 19/100\n",
      "420/420 [==============================] - 246s 586ms/step - loss: 0.6475 - accuracy: 0.6274\n",
      "Epoch 20/100\n",
      "420/420 [==============================] - 251s 598ms/step - loss: 0.6492 - accuracy: 0.6241\n",
      "Epoch 21/100\n",
      "420/420 [==============================] - 249s 592ms/step - loss: 0.6466 - accuracy: 0.6285\n",
      "Epoch 22/100\n",
      "420/420 [==============================] - 244s 580ms/step - loss: 0.6470 - accuracy: 0.6279\n",
      "Epoch 23/100\n",
      "420/420 [==============================] - 239s 568ms/step - loss: 0.6471 - accuracy: 0.6263\n",
      "Epoch 24/100\n",
      "420/420 [==============================] - 242s 575ms/step - loss: 0.6448 - accuracy: 0.6240\n",
      "Epoch 25/100\n",
      "420/420 [==============================] - 243s 578ms/step - loss: 0.6457 - accuracy: 0.6273\n",
      "Epoch 26/100\n",
      "420/420 [==============================] - 248s 590ms/step - loss: 0.6440 - accuracy: 0.6286\n",
      "Epoch 27/100\n",
      "420/420 [==============================] - 203s 483ms/step - loss: 0.6436 - accuracy: 0.6249\n",
      "Epoch 28/100\n",
      "420/420 [==============================] - 164s 391ms/step - loss: 0.6426 - accuracy: 0.6286\n",
      "Epoch 29/100\n",
      "420/420 [==============================] - 165s 394ms/step - loss: 0.6423 - accuracy: 0.6295\n",
      "Epoch 30/100\n",
      "420/420 [==============================] - 165s 392ms/step - loss: 0.6424 - accuracy: 0.6299\n",
      "Epoch 31/100\n",
      "420/420 [==============================] - 165s 394ms/step - loss: 0.6428 - accuracy: 0.6295\n",
      "Epoch 32/100\n",
      "420/420 [==============================] - 166s 395ms/step - loss: 0.6437 - accuracy: 0.6242\n",
      "Epoch 33/100\n",
      "420/420 [==============================] - 173s 412ms/step - loss: 0.6428 - accuracy: 0.6261\n",
      "Epoch 34/100\n",
      "420/420 [==============================] - 170s 404ms/step - loss: 0.6426 - accuracy: 0.6280\n",
      "Epoch 00034: early stopping\n",
      "420/420 [==============================] - 32s 76ms/step - loss: 0.6526 - accuracy: 0.5924\n",
      "train Accuracy: 59.239376\n",
      " 46/207 [=====>........................] - ETA: 11s - loss: 0.6627 - accuracy: 0.572"
     ]
    }
   ],
   "source": [
    "def Average(lst):\n",
    "    return sum(lst) / len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207/207 [==============================] - 8s 37ms/step - loss: 0.6630 - accuracy: 0.5748\n",
      "test Accuracy: 57.478052\n"
     ]
    }
   ],
   "source": [
    "print('test Accuracy: %f' % (Average(acc)))\n",
    "print('test precision: %f' % (Average(pre)))\n",
    "print('test recall: %f' % (Average(rec)))\n",
    "print('test f1_score: %f' % (Average(f1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
